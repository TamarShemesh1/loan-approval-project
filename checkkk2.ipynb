{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project – Introduction to Machine Learning (Group 32)\n",
    "## Loan Approval Prediction – Binary Classification\n",
    "**Tamar & Tala – Spring 2025**\n",
    "\n",
    "This notebook walks through our end-to-end machine learning project:\n",
    "- Data exploration \n",
    "- Feature processing \n",
    "- Model training \n",
    "- Evaluation \n",
    "- Final prediction submission "
   ],
   "id": "382a8f2ce576c407"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Libraries and Data\n",
    "_We’ll start by loading the data and setting up useful libraries._"
   ],
   "id": "809af347f383058e"
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T21:19:40.037212100Z",
     "start_time": "2025-06-02T21:19:39.793374700Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   customer_id   Age Employment_Type       A Education_Level  \\\n0       115892  90.0             NaN   77053         HS-grad   \n1       115893  82.0         Private  132870         HS-grad   \n2       115895  54.0         Private  140359         7th-8th   \n3       115896  41.0         Private  264663    Some-college   \n4       115897  34.0         Private  216864         HS-grad   \n\n   Years_of_Education Marital_Status           Job_Type Household_Role  \\\n0                 9.0        Widowed                NaN  Not-in-family   \n1                 9.0        Widowed    Exec-managerial  Not-in-family   \n2                 4.0       Divorced  Machine-op-inspct      Unmarried   \n3                10.0      Separated     Prof-specialty      Own-child   \n4                 9.0       Divorced      Other-service      Unmarried   \n\n  Ethnicity  ... Investment_Gain  Investment_Loss  Weekly_Work_Hours  \\\n0     White  ...             0.0           4356.0                 40   \n1     White  ...             0.0           4356.0                 18   \n2     White  ...             0.0           3900.0                 40   \n3     White  ...             0.0           3900.0                 40   \n4     White  ...             NaN           3770.0                 45   \n\n   Country_of_Residence           C  Preferred_Communication_Method         D  \\\n0         United-States  104.906221                      Phone_Call  2.865629   \n1         United-States   96.358501                            mail  5.528583   \n2         United-States  115.529631                            mail  3.816915   \n3         United-States   85.732506                             NaN  5.416363   \n4         United-States  115.218443                            Mail  6.453932   \n\n    B           E  Loan_Approval  \n0 NaN  170.887465              0  \n1 NaN  156.630201              0  \n2 NaN  165.635557              0  \n3 NaN  123.379007              0  \n4 NaN  155.262131              0  \n\n[5 rows x 21 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>customer_id</th>\n      <th>Age</th>\n      <th>Employment_Type</th>\n      <th>A</th>\n      <th>Education_Level</th>\n      <th>Years_of_Education</th>\n      <th>Marital_Status</th>\n      <th>Job_Type</th>\n      <th>Household_Role</th>\n      <th>Ethnicity</th>\n      <th>...</th>\n      <th>Investment_Gain</th>\n      <th>Investment_Loss</th>\n      <th>Weekly_Work_Hours</th>\n      <th>Country_of_Residence</th>\n      <th>C</th>\n      <th>Preferred_Communication_Method</th>\n      <th>D</th>\n      <th>B</th>\n      <th>E</th>\n      <th>Loan_Approval</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>115892</td>\n      <td>90.0</td>\n      <td>NaN</td>\n      <td>77053</td>\n      <td>HS-grad</td>\n      <td>9.0</td>\n      <td>Widowed</td>\n      <td>NaN</td>\n      <td>Not-in-family</td>\n      <td>White</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>4356.0</td>\n      <td>40</td>\n      <td>United-States</td>\n      <td>104.906221</td>\n      <td>Phone_Call</td>\n      <td>2.865629</td>\n      <td>NaN</td>\n      <td>170.887465</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>115893</td>\n      <td>82.0</td>\n      <td>Private</td>\n      <td>132870</td>\n      <td>HS-grad</td>\n      <td>9.0</td>\n      <td>Widowed</td>\n      <td>Exec-managerial</td>\n      <td>Not-in-family</td>\n      <td>White</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>4356.0</td>\n      <td>18</td>\n      <td>United-States</td>\n      <td>96.358501</td>\n      <td>mail</td>\n      <td>5.528583</td>\n      <td>NaN</td>\n      <td>156.630201</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>115895</td>\n      <td>54.0</td>\n      <td>Private</td>\n      <td>140359</td>\n      <td>7th-8th</td>\n      <td>4.0</td>\n      <td>Divorced</td>\n      <td>Machine-op-inspct</td>\n      <td>Unmarried</td>\n      <td>White</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>3900.0</td>\n      <td>40</td>\n      <td>United-States</td>\n      <td>115.529631</td>\n      <td>mail</td>\n      <td>3.816915</td>\n      <td>NaN</td>\n      <td>165.635557</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>115896</td>\n      <td>41.0</td>\n      <td>Private</td>\n      <td>264663</td>\n      <td>Some-college</td>\n      <td>10.0</td>\n      <td>Separated</td>\n      <td>Prof-specialty</td>\n      <td>Own-child</td>\n      <td>White</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>3900.0</td>\n      <td>40</td>\n      <td>United-States</td>\n      <td>85.732506</td>\n      <td>NaN</td>\n      <td>5.416363</td>\n      <td>NaN</td>\n      <td>123.379007</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>115897</td>\n      <td>34.0</td>\n      <td>Private</td>\n      <td>216864</td>\n      <td>HS-grad</td>\n      <td>9.0</td>\n      <td>Divorced</td>\n      <td>Other-service</td>\n      <td>Unmarried</td>\n      <td>White</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>3770.0</td>\n      <td>45</td>\n      <td>United-States</td>\n      <td>115.218443</td>\n      <td>Mail</td>\n      <td>6.453932</td>\n      <td>NaN</td>\n      <td>155.262131</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 21 columns</p>\n</div>"
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve, classification_report\n",
    "\n",
    "import missingno as msno\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "\n",
    "\n",
    "# Load the data\n",
    "train = pd.read_csv('train.csv', na_values=[\"?\"])\n",
    "test = pd.read_csv('test.csv', na_values=[\"?\"])\n",
    "train.head()"
   ],
   "id": "306e676c18666e27"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis (EDA)\n",
    "_Let's understand the data structure, spot missing values, and explore correlations._\n",
    "\n",
    "\n",
    "**Answer in markdown:**\n",
    "- Are there outliers?\n",
    "- Are there missing values?\n",
    "- Any early ideas on important features?"
   ],
   "id": "1fafd0a08e17e5a7"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set dimensions: 27676 rows × 21 columns\n"
     ]
    }
   ],
   "source": [
    "n_rows,n_columns = train.shape\n",
    "print(f\"Training set dimensions: {n_rows} rows × {n_columns} columns\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-02T05:51:57.548722700Z",
     "start_time": "2025-06-02T05:51:57.505675800Z"
    }
   },
   "id": "511a2b2c7c2410bf"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "         customer_id           Age             A  Years_of_Education  \\\ncount   27676.000000  27541.000000  2.767600e+04        27122.000000   \nmean   132196.289854     38.597727  1.898849e+05           10.079972   \nstd      9399.839425     13.636965  1.053593e+05            2.575332   \nmin    115892.000000     17.000000  1.228500e+04            1.000000   \n25%    124043.750000     28.000000  1.177652e+05            9.000000   \n50%    132222.500000     37.000000  1.785870e+05           10.000000   \n75%    140322.500000     48.000000  2.376340e+05           12.000000   \nmax    148452.000000     90.000000  1.484705e+06           16.000000   \n\n       Investment_Gain  Investment_Loss  Weekly_Work_Hours             C  \\\ncount     26846.000000     27647.000000       27676.000000  26561.000000   \nmean       1091.680101        86.084458          40.482151     99.957601   \nstd        7482.292675       400.504134          12.371156      9.911054   \nmin           0.000000         0.000000           1.000000     58.136513   \n25%           0.000000         0.000000          40.000000     93.273986   \n50%           0.000000         0.000000          40.000000     99.903569   \n75%           0.000000         0.000000          45.000000    106.636563   \nmax       99999.000000      4356.000000          99.000000    144.309842   \n\n                  D    B             E  Loan_Approval  \ncount  27676.000000  0.0  24389.000000   27676.000000  \nmean       4.987450  NaN    139.251615       0.240786  \nstd        1.511751  NaN     13.708225       0.427569  \nmin       -1.711896  NaN     82.675954       0.000000  \n25%        3.968762  NaN    129.783073       0.000000  \n50%        4.994815  NaN    139.078247       0.000000  \n75%        5.999440  NaN    148.488675       0.000000  \nmax       10.876204  NaN    199.134411       1.000000  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>customer_id</th>\n      <th>Age</th>\n      <th>A</th>\n      <th>Years_of_Education</th>\n      <th>Investment_Gain</th>\n      <th>Investment_Loss</th>\n      <th>Weekly_Work_Hours</th>\n      <th>C</th>\n      <th>D</th>\n      <th>B</th>\n      <th>E</th>\n      <th>Loan_Approval</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>27676.000000</td>\n      <td>27541.000000</td>\n      <td>2.767600e+04</td>\n      <td>27122.000000</td>\n      <td>26846.000000</td>\n      <td>27647.000000</td>\n      <td>27676.000000</td>\n      <td>26561.000000</td>\n      <td>27676.000000</td>\n      <td>0.0</td>\n      <td>24389.000000</td>\n      <td>27676.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>132196.289854</td>\n      <td>38.597727</td>\n      <td>1.898849e+05</td>\n      <td>10.079972</td>\n      <td>1091.680101</td>\n      <td>86.084458</td>\n      <td>40.482151</td>\n      <td>99.957601</td>\n      <td>4.987450</td>\n      <td>NaN</td>\n      <td>139.251615</td>\n      <td>0.240786</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>9399.839425</td>\n      <td>13.636965</td>\n      <td>1.053593e+05</td>\n      <td>2.575332</td>\n      <td>7482.292675</td>\n      <td>400.504134</td>\n      <td>12.371156</td>\n      <td>9.911054</td>\n      <td>1.511751</td>\n      <td>NaN</td>\n      <td>13.708225</td>\n      <td>0.427569</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>115892.000000</td>\n      <td>17.000000</td>\n      <td>1.228500e+04</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>58.136513</td>\n      <td>-1.711896</td>\n      <td>NaN</td>\n      <td>82.675954</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>124043.750000</td>\n      <td>28.000000</td>\n      <td>1.177652e+05</td>\n      <td>9.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>40.000000</td>\n      <td>93.273986</td>\n      <td>3.968762</td>\n      <td>NaN</td>\n      <td>129.783073</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>132222.500000</td>\n      <td>37.000000</td>\n      <td>1.785870e+05</td>\n      <td>10.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>40.000000</td>\n      <td>99.903569</td>\n      <td>4.994815</td>\n      <td>NaN</td>\n      <td>139.078247</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>140322.500000</td>\n      <td>48.000000</td>\n      <td>2.376340e+05</td>\n      <td>12.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>45.000000</td>\n      <td>106.636563</td>\n      <td>5.999440</td>\n      <td>NaN</td>\n      <td>148.488675</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>148452.000000</td>\n      <td>90.000000</td>\n      <td>1.484705e+06</td>\n      <td>16.000000</td>\n      <td>99999.000000</td>\n      <td>4356.000000</td>\n      <td>99.000000</td>\n      <td>144.309842</td>\n      <td>10.876204</td>\n      <td>NaN</td>\n      <td>199.134411</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-02T06:11:53.621972900Z",
     "start_time": "2025-06-02T06:11:53.544206300Z"
    }
   },
   "id": "92e56be4784cad8a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train.info()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-01T12:48:38.046148100Z",
     "start_time": "2025-06-01T12:48:38.046148100Z"
    }
   },
   "id": "6bd74af3cb16574"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Column B is completely empty (100% missing), so we will remove it from the dataset.\n",
    "Several columns, such as Employment_Type, Job_Type and Ethnicity have a high percentage of missing values.\n",
    "We'll need to decide whether to fill them or drop them."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9f58dd8d06540027"
  },
  {
   "cell_type": "markdown",
   "source": [
    "_Visualizing the variables to check their distributions:_"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "49bbe0e7870b3c7e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Histograms and count plots of each explanatory variable\n",
    "\n",
    "for col in train.columns:\n",
    "    plt.figure(figsize=(15, 4))\n",
    "    \n",
    "    # For numeric variables\n",
    "    if pd.api.types.is_numeric_dtype(train[col]):\n",
    "        sns.histplot(train[col].dropna(), kde=True)\n",
    "        plt.title(f\"Distribution of {col}\")\n",
    "    \n",
    "    # For categorical variables \n",
    "    else:\n",
    "        sns.countplot(x=col, data=train, order=train[col].value_counts().index)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.title(f\"Countplot of {col}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2f8d6bf1fc5b57f0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Bar plots of approval rates for categorical variables\n",
    "\n",
    "categorical_vars = [\n",
    "    'Employment_Type', 'Education_Level', 'Marital_Status', 'Job_Type',\n",
    "    'Household_Role', 'Ethnicity', 'Gender', 'Country_of_Residence',\n",
    "    'Preferred_Communication_Method'\n",
    "]\n",
    "\n",
    "for col in categorical_vars:\n",
    "    plt.figure(figsize=(15, 4))\n",
    "    sns.barplot(x=col, y='Loan_Approval', data=train, estimator='mean', order=train[col].value_counts().index)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.title(f'Approval Rate by {col}')\n",
    "    plt.ylabel('Approval Rate (Mean of Loan_approval)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-01T12:48:38.073193700Z",
     "start_time": "2025-06-01T12:48:38.051477200Z"
    }
   },
   "id": "42a31bcdb5f5d849"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Box plots of approved vs. rejected for numerical variables\n",
    "train['Loan_Approval_str'] = train['Loan_Approval'].map({0: 'Rejected', 1: 'Approved'})\n",
    "\n",
    "# Remove columns that shouldn't be plotted\n",
    "numeric_cols = [col for col in train.select_dtypes(include='number').columns if col not in ['customer_id', 'Loan_Approval']]\n",
    "\n",
    "# Create one boxplot per feature vs Loan Approval\n",
    "for col in numeric_cols:\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.boxplot(x='Loan_Approval_str', y=col, data=train)\n",
    "    plt.title(f'{col} by Loan Approval')\n",
    "    plt.xlabel('Loan Approval')\n",
    "    plt.ylabel(col)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-06-01T12:48:38.055410500Z"
    }
   },
   "id": "343249a9cee851ed"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Key Insights from Feature Distributions**\n",
    "1. Employment_Type, Country_of_Residence and Ethnicity are highly imbalanced.\n",
    "To reduce sparsity and improve generalization, we will group the less frequent categories into broader, more meaningful groups.\n",
    "2. A is right-skewed, so a log transformation may help normalize its distribution.\n",
    "3. Investment_Gain and Investment_Loss are extremely skewed with large outliers, so a log transformation might help, and merging them into a single Net_Investment feature may provide a clearer signal.\n",
    "4. Preferred_Communication_Method has inconsistent values due to variations in capitalization and wording (e.g., \"Email\" vs \"email\"), so we unified similar values.\n",
    "5. Based on the approval rate bar plot for Household_Role, we observed clear patterns across the categories.\n",
    "We grouped them into three broader categories with similar approval behavior to simplify the feature and improve model stability.\n",
    "6. Column B is completely empty, so we decided to remove it.\n",
    "7.  We simplified the Country_of_Residence feature by grouping it into three categories: USA, Mexico, and Other. The vast majority of people were from the United States, and the approval rate among other countries wasn't very different.\n",
    "8.  We simplified the Employment_Type feature by grouping it into four categories: Private, not private and, and non-working.  The vast majority of people were private, and the noticeable difference in the approval rate was between non-private to non-working.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9e012bc51fce0e55"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Creating a heatmap\n",
    "\n",
    "# Select only numeric columns\n",
    "numeric_df = train.select_dtypes(include='number')\n",
    "\n",
    "# Compute the correlation matrix\n",
    "correlation_matrix = numeric_df.corr()\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", square=True)\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-06-01T12:48:38.058449700Z"
    }
   },
   "id": "4198104465d53ee8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Checking which variables are most correlated with Loan_approval (only linear) - this is the Loan_Approval row in the heatmap\n",
    "numeric_df = train.select_dtypes(include='number')\n",
    "correlations = numeric_df.corr()['Loan_Approval'].sort_values(ascending=False)\n",
    "print(correlations)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-06-01T12:48:38.060444500Z"
    }
   },
   "id": "dd0e681b69d85555"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Key Insights from the heat map**\n",
    "C and E are highly correlated (r = 0.87), indicating redundancy.\n",
    "Neither has a strong linear correlation with the target, but E shows a slightly stronger signal (r = 0.125). Even though E has more missing values (as we will see later on), it’s still more useful, so we’ll keep E and drop C. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "18833f3e4451f9f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We checked for overlap between Employment_Type and Job_Type using a cross-tabulation heatmap.\n",
    "The results show no strong or consistent relationship between the two — most employment types are associated with a wide variety of job types.\n",
    "Therefore, we concluded that the two features capture distinct information and chose to keep both."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dbba21d75f846898"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "crosstab = pd.crosstab(train['Employment_Type'], train['Job_Type'], normalize='index')\n",
    "plt.figure(figsize=(14, 7))\n",
    "sns.heatmap(crosstab, cmap=\"Blues\", annot=True, fmt=\".2f\")\n",
    "plt.title(\"Employment Type vs Job Type\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-06-01T12:48:38.064433800Z"
    }
   },
   "id": "2c2ad593ba0cad71"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Changes based on the key insights:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "114b5f5a38536405"
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [],
   "source": [
    "# 1. Removing columns B and C\n",
    "train = train.drop(['B', 'C'], axis=1)\n",
    "test = test.drop(['B', 'C'], axis=1)\n",
    "n_columns -= 2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-02T21:19:54.581759300Z",
     "start_time": "2025-06-02T21:19:54.513424600Z"
    }
   },
   "id": "e60c0335a6987d0f"
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [],
   "source": [
    "# ------------ Grouping functions ------------\n",
    "\n",
    "# Grouping Preferred_Communication_Method\n",
    "def group_pref_com(pref):\n",
    "    if pref == \"Mail\" or pref == \"mail\":\n",
    "        return \"Mail\"\n",
    "    if pref == \"Email\" or pref == \"email\":\n",
    "        return \"EMail\"\n",
    "    if pref == \"Phone\" or pref == \"phone\" or pref == \"phone call\":\n",
    "        return \"Phone\"\n",
    "    else:\n",
    "        return \"Other\"\n",
    "\n",
    "# Grouping Country_of_Residence\n",
    "def group_country(country):\n",
    "    if country == \"United-States\":\n",
    "        return \"USA\"\n",
    "    elif country == \"Mexico\":\n",
    "        return \"Mexico\"\n",
    "    else:\n",
    "        return \"Other\"\n",
    "    \n",
    "# Grouping Education_Level\n",
    "def group_education(edu):\n",
    "    pre_school = ['Preschool']\n",
    "    school = ['1st-4th', '5th-6th', '9th', '10th', '11th', '12th', '7th-8th']\n",
    "    post_hs = ['HS-grad', 'Some-college']\n",
    "    assoc = ['Assoc-voc', 'Assoc-acdm']\n",
    "    higher = ['Masters', 'Bachelors']\n",
    "    postgraduate = ['Doctorate', 'Prof-school']\n",
    "    \n",
    "    if edu in pre_school:\n",
    "        return 'pre_school'\n",
    "    elif edu in school:\n",
    "        return 'school'\n",
    "    elif edu in post_hs:\n",
    "        return 'post_hs'\n",
    "    elif edu in assoc:\n",
    "        return 'assoc'\n",
    "    elif edu in higher:\n",
    "        return 'higher'\n",
    "    elif edu in postgraduate:\n",
    "        return 'postgraduate'\n",
    "    else:\n",
    "        return 'Other'\n",
    "    \n",
    "# Grouping Household_role\n",
    "def group_household_role(role):\n",
    "    if role in ['Husband', 'Wife']:\n",
    "        return 'Spouse'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "# Grouping Employment_Type\n",
    "def group_employment(emp):\n",
    "    if emp == 'Private':\n",
    "        return 'Private'\n",
    "    elif emp in ['Self-emp-not-inc', 'Self-emp-inc', 'Local-gov', 'State-gov', 'Federal-gov']:\n",
    "        return 'Not-Private'\n",
    "    elif emp in ['Without-pay', 'Never-worked']:\n",
    "        return 'Non-working'\n",
    "    else:\n",
    "        return 'Other'\n",
    "    \n",
    "# Grouping Ethnicity:\n",
    "def group_ethnicity(eth):\n",
    "    if eth == 'White':\n",
    "        return 'White'\n",
    "    elif eth == 'Black':\n",
    "        return 'Black'\n",
    "    else:\n",
    "        return 'Other'\n",
    "    \n",
    "# 1. Mapping dictionary for Years_of_Education\n",
    "edu_to_years = {\n",
    "    'Preschool'    : 0,\n",
    "    '1st-4th'      : (1 + 2 + 3 + 4) / 4,\n",
    "    '5th-6th'      : 5.5,\n",
    "    '7th-8th'      : 7.5,\n",
    "    '9th'          : 9,\n",
    "    '10th'         : 10,\n",
    "    '11th'         : 11,\n",
    "    '12th'         : 12,\n",
    "    'HS-grad'      : 12,\n",
    "    'Some-college' : (12 + 13 + 14 + 15) / 4,\n",
    "    'Assoc-voc'    : 14,\n",
    "    'Assoc-acdm'   : 14,\n",
    "    'Bachelors'    : 16,\n",
    "    'Masters'      : 18,\n",
    "    'Prof-school'  : 19,\n",
    "    'Doctorate'    : 20\n",
    "}\n",
    "\n",
    "'''\n",
    "def group_edu_years(X):\n",
    "    #X is a 2D array: [Years_of_Education, Education_Level\n",
    "    \n",
    "    if X.shape[1] != 2:\n",
    "        raise ValueError(\"group_edu_years expects exactly 2 columns: [Years_of_Education, Education_Level]\")\n",
    "\n",
    "    # Convert columns to panda series to be able to map\n",
    "    years_of_education = pd.Series(X[:, 0])\n",
    "    education_level = pd.Series(X[:, 1])\n",
    "    \n",
    "    # Map Education_Level → numeric years\n",
    "    mapped_years = education_level.map(edu_to_years)\n",
    "    \n",
    "    # Fill NaN in the original 'years' with the mapped values\n",
    "    filled = years_of_education.fillna(mapped_years)\n",
    "    \n",
    "    # Return as a single-column DataFrame\n",
    "    return filled.to_frame(name='Years_of_Education')\n",
    "'''\n",
    "def group_edu_years(X):\n",
    "    # Ensure X is treated as a 2D array\n",
    "    arr = X.to_numpy()\n",
    "    \n",
    "    if arr.ndim != 2 or arr.shape[1] != 2:\n",
    "        raise ValueError(\"group_edu_years expects exactly 2 columns: [Years_of_Education, Education_Level]\")\n",
    "    \n",
    "    # Now arr[:, 0] and arr[:, 1] work as intended\n",
    "    raw_years   = pd.Series(arr[:, 0])\n",
    "    raw_levels  = pd.Series(arr[:, 1])\n",
    "    \n",
    "    # Map Education_Level → numeric years\n",
    "    mapped_years = raw_levels.map(edu_to_years)\n",
    "    \n",
    "    # Fill NaN in raw_years with mapped_years\n",
    "    filled = raw_years.fillna(mapped_years)\n",
    "    \n",
    "    # Return a single-column DataFrame\n",
    "    return filled.to_frame(name=\"Years_of_Education\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-02T21:19:55.091658700Z",
     "start_time": "2025-06-02T21:19:55.043559300Z"
    }
   },
   "id": "ea59906540420095"
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [],
   "source": [
    "'''from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "# ------------ function transformers ------------\n",
    "\n",
    "pref_com_group = FunctionTransformer(\n",
    "    func=lambda X: pd.Series(X.ravel()).map(group_pref_com).to_frame(),\n",
    "    validate=False\n",
    ")\n",
    "\n",
    "country_group = FunctionTransformer(\n",
    "    func=lambda X: pd.Series(X.ravel()).map(group_country).to_frame(),\n",
    "    validate=False\n",
    ")\n",
    "\n",
    "edu_level_group = FunctionTransformer(\n",
    "    func=lambda X: pd.Series(X.ravel()).map(group_education).to_frame(),\n",
    "    validate=False\n",
    ")\n",
    "\n",
    "house_hold_group = FunctionTransformer(\n",
    "    func=lambda X: pd.Series(X.ravel()).map(group_household_role).to_frame(),\n",
    "    validate=False\n",
    ")\n",
    "\n",
    "emp_type_group = FunctionTransformer(\n",
    "    func=lambda X: pd.Series(X.ravel()).map(group_employment).to_frame(),\n",
    "    validate=False\n",
    ")\n",
    "\n",
    "ethnicity_group = FunctionTransformer(\n",
    "    func=lambda X: pd.Series(X.ravel()).map(group_ethnicity).to_frame(),\n",
    "    validate=False\n",
    ")\n",
    "\n",
    "edu_years_group = FunctionTransformer(\n",
    "    func=lambda X: pd.Series(X.ravel()).map(group_edu_years).to_frame(),\n",
    "    validate=False\n",
    ")\n",
    "'''\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Single-column groupers: convert X (which may be a 1-col DataFrame or 2D array shape (n,1))\n",
    "#    into a flat 1D array via X.to_numpy().ravel(), then map the scalar function.\n",
    "\n",
    "pref_com_group = FunctionTransformer(\n",
    "    func=lambda X: pd.Series(\n",
    "        X.to_numpy().ravel()\n",
    "    ).map(group_pref_com).to_frame(\"Preferred_Communication_Method\"),\n",
    "    validate=False\n",
    ")\n",
    "\n",
    "country_group = FunctionTransformer(\n",
    "    func=lambda X: pd.Series(\n",
    "        X.to_numpy().ravel()\n",
    "    ).map(group_country).to_frame(\"Country_of_Residence\"),\n",
    "    validate=False\n",
    ")\n",
    "\n",
    "edu_level_group = FunctionTransformer(\n",
    "    func=lambda X: pd.Series(\n",
    "        X.to_numpy().ravel()\n",
    "    ).map(group_education).to_frame(\"Education_Level_Mapped\"),\n",
    "    validate=False\n",
    ")\n",
    "\n",
    "house_hold_group = FunctionTransformer(\n",
    "    func=lambda X: pd.Series(\n",
    "        X.to_numpy().ravel()\n",
    "    ).map(group_household_role).to_frame(\"Household_Role\"),\n",
    "    validate=False\n",
    ")\n",
    "\n",
    "emp_type_group = FunctionTransformer(\n",
    "    func=lambda X: pd.Series(\n",
    "        X.to_numpy().ravel()\n",
    "    ).map(group_employment).to_frame(\"Employment_Type\"),\n",
    "    validate=False\n",
    ")\n",
    "\n",
    "ethnicity_group = FunctionTransformer(\n",
    "    func=lambda X: pd.Series(\n",
    "        X.to_numpy().ravel()\n",
    "    ).map(group_ethnicity).to_frame(\"Ethnicity\"),\n",
    "    validate=False\n",
    ")\n",
    "\n",
    "\n",
    "# 2. Two-column grouper: \n",
    "#    X will be shape (n_samples, 2), where column 0 = Years_of_Education,\n",
    "#    column 1 = Education_Level. We need to call group_edu_years on each row.\n",
    "\n",
    "edu_years_group = FunctionTransformer(\n",
    "    func=lambda X: group_edu_years(X),\n",
    "    validate=False\n",
    ")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-02T21:19:55.976731700Z",
     "start_time": "2025-06-02T21:19:55.908278500Z"
    }
   },
   "id": "323cef953dcfe6f8"
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [],
   "source": [
    "# ------------ pipelines ------------\n",
    "\n",
    "knn_imputer = KNNImputer(n_neighbors=3, weights='uniform')\n",
    "\n",
    "# Preferred_Communication_Method pipeline\n",
    "pref_pipeline = Pipeline([\n",
    "    ('group', pref_com_group),  \n",
    "    ('impute', SimpleImputer(strategy='constant', fill_value='Missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Country_of_Residence pipeline\n",
    "country_pipeline = Pipeline([\n",
    "    ('group', country_group),\n",
    "    ('impute', SimpleImputer(strategy='most_frequent')),  # or 'constant' if you prefer\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Education_Level pipeline\n",
    "edu_pipeline = Pipeline([\n",
    "    ('group', edu_level_group),\n",
    "    ('impute', SimpleImputer(strategy='constant', fill_value='Missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Employment_Type pipeline\n",
    "employment_pipeline = Pipeline([\n",
    "    ('group', emp_type_group),\n",
    "    ('impute', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Ethnicity pipeline\n",
    "ethnicity_pipeline = Pipeline([\n",
    "    ('group', ethnicity_group),\n",
    "    ('impute', SimpleImputer(strategy='constant', fill_value='Missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Household pipeline\n",
    "Household_pipeline = Pipeline([\n",
    "    ('group', house_hold_group),\n",
    "    ('impute', SimpleImputer(strategy='constant', fill_value='Missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Years of education pipeline\n",
    "edu_years_pipeline = Pipeline([\n",
    "    ('fill_years', edu_years_group),\n",
    "    ('impute_years', SimpleImputer(strategy='mean'))\n",
    "])\n",
    "\n",
    "\n",
    "# columns without special adjustments:\n",
    "categorical_col = ['Gender', 'Marital_Status', 'Job_Type', 'Employment_Type']\n",
    "numerical_col = ['E']\n",
    "skw_numerical_col = ['Age', 'Investment_Gain', 'Investment_Loss']\n",
    "\n",
    "# Pipeline for categorical features\n",
    "cat_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='Missing')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Pipeline for numerical features (mean imputation)\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean'))\n",
    "])\n",
    "\n",
    "# Pipeline for numerical features (median imputation) - for skewed columns\n",
    "skw_numerical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median'))\n",
    "])\n",
    "\n",
    "\n",
    "# Combine pipelines into a ColumnTransformer\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('pref_communication',       pref_pipeline,         ['Preferred_Communication_Method']),\n",
    "    ('country',    country_pipeline,      ['Country_of_Residence']),\n",
    "    ('edu_level',  edu_pipeline,          ['Education_Level']),\n",
    "    ('edu_years',  edu_years_pipeline,    ['Years_of_Education', 'Education_Level']),\n",
    "    ('employment_type', employment_pipeline,   ['Employment_Type']),\n",
    "    ('ethnicity',  ethnicity_pipeline,    ['Ethnicity']),\n",
    "    ('household_role',  Household_pipeline,    ['Household_Role']),\n",
    "    ('cat',  cat_pipeline,          ['Gender', 'Marital_Status', 'Job_Type', 'Employment_Type']), # Other categorical columns\n",
    "    ('num',  num_pipeline,          ['E']),                                                       # Simple numeric column\n",
    "    ('skw_num',    skw_numerical_pipeline,['Age', 'Investment_Gain', 'Investment_Loss'])                # Skewed numeric columns    \n",
    "    ],\n",
    "    remainder=\"passthrough\"\n",
    "    # All remaining columns that were not specified in transformer. These columns don't need any transformation. This subset of columns is concatenated with the output of the transformers.\n",
    ")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-02T21:19:56.816982200Z",
     "start_time": "2025-06-02T21:19:56.740673700Z"
    }
   },
   "id": "ba45e67358850d1a"
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Preferred_Communication_Method_EMail', 'Preferred_Communication_Method_Mail', 'Preferred_Communication_Method_Other', 'Preferred_Communication_Method_Phone', 'Country_of_Residence_Mexico', 'Country_of_Residence_Other', 'Country_of_Residence_USA', 'Education_Level_assoc', 'Education_Level_higher', 'Education_Level_post_hs', 'Education_Level_postgraduate', 'Education_Level_pre_school', 'Education_Level_school', 'Years_of_Education', 'Employment_Type_Non-working', 'Employment_Type_Not-Private', 'Employment_Type_Other', 'Employment_Type_Private', 'Ethnicity_Black', 'Ethnicity_Other', 'Ethnicity_White', 'Household_Role_Other', 'Household_Role_Spouse', 'Gender_Female', 'Gender_Male', 'Marital_Status_Divorced', 'Marital_Status_Married-AF-spouse', 'Marital_Status_Married-civ-spouse', 'Marital_Status_Married-spouse-absent', 'Marital_Status_Never-married', 'Marital_Status_Separated', 'Marital_Status_Widowed', 'Job_Type_Adm-clerical', 'Job_Type_Armed-Forces', 'Job_Type_Craft-repair', 'Job_Type_Exec-managerial', 'Job_Type_Farming-fishing', 'Job_Type_Handlers-cleaners', 'Job_Type_Machine-op-inspct', 'Job_Type_Missing', 'Job_Type_Other-service', 'Job_Type_Priv-house-serv', 'Job_Type_Prof-specialty', 'Job_Type_Protective-serv', 'Job_Type_Sales', 'Job_Type_Tech-support', 'Job_Type_Transport-moving', 'Employment_Type_Federal-gov', 'Employment_Type_Local-gov', 'Employment_Type_Missing', 'Employment_Type_Never-worked', 'Employment_Type_Private', 'Employment_Type_Self-emp-inc', 'Employment_Type_Self-emp-not-inc', 'Employment_Type_State-gov', 'Employment_Type_Without-pay', 'E', 'Age', 'Investment_Gain', 'Investment_Loss', 'customer_id', 'A', 'Weekly_Work_Hours', 'D', 'Loan_Approval']\n",
      "65\n"
     ]
    },
    {
     "data": {
      "text/plain": "   Preferred_Communication_Method_EMail  Preferred_Communication_Method_Mail  \\\n0                                   0.0                                  0.0   \n1                                   0.0                                  1.0   \n2                                   0.0                                  1.0   \n3                                   0.0                                  0.0   \n4                                   0.0                                  1.0   \n\n   Preferred_Communication_Method_Other  Preferred_Communication_Method_Phone  \\\n0                                   1.0                                   0.0   \n1                                   0.0                                   0.0   \n2                                   0.0                                   0.0   \n3                                   1.0                                   0.0   \n4                                   0.0                                   0.0   \n\n   Country_of_Residence_Mexico  Country_of_Residence_Other  \\\n0                          0.0                         0.0   \n1                          0.0                         0.0   \n2                          0.0                         0.0   \n3                          0.0                         0.0   \n4                          0.0                         0.0   \n\n   Country_of_Residence_USA  Education_Level_assoc  Education_Level_higher  \\\n0                       1.0                    0.0                     0.0   \n1                       1.0                    0.0                     0.0   \n2                       1.0                    0.0                     0.0   \n3                       1.0                    0.0                     0.0   \n4                       1.0                    0.0                     0.0   \n\n   Education_Level_post_hs  ...  Employment_Type_Without-pay           E  \\\n0                      1.0  ...                          0.0  170.887465   \n1                      1.0  ...                          0.0  156.630201   \n2                      0.0  ...                          0.0  165.635557   \n3                      1.0  ...                          0.0  123.379007   \n4                      1.0  ...                          0.0  155.262131   \n\n    Age  Investment_Gain  Investment_Loss  customer_id         A  \\\n0  90.0              0.0           4356.0     115892.0   77053.0   \n1  82.0              0.0           4356.0     115893.0  132870.0   \n2  54.0              0.0           3900.0     115895.0  140359.0   \n3  41.0              0.0           3900.0     115896.0  264663.0   \n4  34.0              0.0           3770.0     115897.0  216864.0   \n\n   Weekly_Work_Hours         D  Loan_Approval  \n0               40.0  2.865629            0.0  \n1               18.0  5.528583            0.0  \n2               40.0  3.816915            0.0  \n3               40.0  5.416363            0.0  \n4               45.0  6.453932            0.0  \n\n[5 rows x 65 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Preferred_Communication_Method_EMail</th>\n      <th>Preferred_Communication_Method_Mail</th>\n      <th>Preferred_Communication_Method_Other</th>\n      <th>Preferred_Communication_Method_Phone</th>\n      <th>Country_of_Residence_Mexico</th>\n      <th>Country_of_Residence_Other</th>\n      <th>Country_of_Residence_USA</th>\n      <th>Education_Level_assoc</th>\n      <th>Education_Level_higher</th>\n      <th>Education_Level_post_hs</th>\n      <th>...</th>\n      <th>Employment_Type_Without-pay</th>\n      <th>E</th>\n      <th>Age</th>\n      <th>Investment_Gain</th>\n      <th>Investment_Loss</th>\n      <th>customer_id</th>\n      <th>A</th>\n      <th>Weekly_Work_Hours</th>\n      <th>D</th>\n      <th>Loan_Approval</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>170.887465</td>\n      <td>90.0</td>\n      <td>0.0</td>\n      <td>4356.0</td>\n      <td>115892.0</td>\n      <td>77053.0</td>\n      <td>40.0</td>\n      <td>2.865629</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>156.630201</td>\n      <td>82.0</td>\n      <td>0.0</td>\n      <td>4356.0</td>\n      <td>115893.0</td>\n      <td>132870.0</td>\n      <td>18.0</td>\n      <td>5.528583</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>165.635557</td>\n      <td>54.0</td>\n      <td>0.0</td>\n      <td>3900.0</td>\n      <td>115895.0</td>\n      <td>140359.0</td>\n      <td>40.0</td>\n      <td>3.816915</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>123.379007</td>\n      <td>41.0</td>\n      <td>0.0</td>\n      <td>3900.0</td>\n      <td>115896.0</td>\n      <td>264663.0</td>\n      <td>40.0</td>\n      <td>5.416363</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>155.262131</td>\n      <td>34.0</td>\n      <td>0.0</td>\n      <td>3770.0</td>\n      <td>115897.0</td>\n      <td>216864.0</td>\n      <td>45.0</td>\n      <td>6.453932</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 65 columns</p>\n</div>"
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw = train.copy()\n",
    "\n",
    "# 1. Fit your preprocessor on the raw data (if you haven't already)\n",
    "#    - If you already ran `preprocessor.fit(...)` earlier, you can skip\n",
    "#      the fit step and go straight to transform().\n",
    "preprocessor.fit(df_raw)\n",
    "\n",
    "# 2. Transform the data to get a NumPy array of size (n_samples, n_total_features)\n",
    "X_transformed = preprocessor.transform(df_raw)\n",
    "\n",
    "# 2. Build a list of all columns explicitly listed in the transformer specs\n",
    "listed_columns = []\n",
    "for _, _, cols in preprocessor.transformers_:\n",
    "    # 'cols' can be a list of column names or a slice object\n",
    "    if isinstance(cols, (list, tuple, np.ndarray)):\n",
    "        listed_columns.extend(list(cols))\n",
    "    # If it were a slice or boolean mask, you’d handle that here.\n",
    "    # In most typical uses, it's a list of strings.\n",
    "\n",
    "listed_columns = set(listed_columns)  # deduplicate\n",
    "\n",
    "# 3. Iterate again to collect output names for each transformer block\n",
    "feature_names = []\n",
    "\n",
    "for name, transformer, cols in preprocessor.transformers_:\n",
    "    # a) If the transformer is explicitly \"drop\", skip it\n",
    "    if transformer == \"drop\":\n",
    "        continue\n",
    "\n",
    "    # b) If the transformer is \"passthrough\", we'll handle it in step 4\n",
    "    #    Actually, ColumnTransformer never lists \"passthrough\" here;\n",
    "    #    remainder=\"passthrough\" is handled separately.\n",
    "    #    So we can ignore that case in this loop.\n",
    "\n",
    "    # c) Figure out the “final” estimator within this transformer\n",
    "    if isinstance(transformer, Pipeline):\n",
    "        final_step = transformer.steps[-1][1]\n",
    "    else:\n",
    "        final_step = transformer\n",
    "\n",
    "    # d) If the final step has get_feature_names_out, call it\n",
    "    if hasattr(final_step, \"get_feature_names_out\"):\n",
    "        try:\n",
    "            names_out = final_step.get_feature_names_out(cols)\n",
    "        except Exception:\n",
    "            # If it errors (e.g. SimpleImputer in older sklearn), fallback:\n",
    "            names_out = list(cols)  # one‐to‐one with input column(s)\n",
    "    else:\n",
    "        # If no get_feature_names_out, assume it returned exactly one column per input\n",
    "        names_out = list(cols)\n",
    "\n",
    "    feature_names.extend(names_out)\n",
    "\n",
    "# The feature_names list contains integer indices for passthrough columns.\n",
    "# We need to replace those integers with the actual column names from df_raw.\n",
    "\n",
    "fixed_feature_names = []\n",
    "for name in feature_names:\n",
    "    if isinstance(name, int):\n",
    "        # look up the real column name by positional index\n",
    "        fixed_feature_names.append(df_raw.columns[name])\n",
    "    else:\n",
    "        fixed_feature_names.append(name)\n",
    "fixed_feature_names.remove(\"Education_Level\")       # Appears twice because of the education mapping\n",
    "        \n",
    "print(fixed_feature_names)\n",
    "\n",
    "\n",
    "# 4. Finally, handle remainder=\"passthrough\" columns\n",
    "'''\n",
    "if preprocessor.remainder == \"passthrough\":\n",
    "    # Find all columns in df_raw that were never listed above\n",
    "    all_raw_cols = list(df_raw.columns)\n",
    "    passthrough_cols = [c for c in all_raw_cols if c not in listed_columns]\n",
    "    fixed_feature_names.extend(passthrough_cols)\n",
    "'''\n",
    "\n",
    "print(len(fixed_feature_names))\n",
    "\n",
    "\n",
    "# 5. Sanity check\n",
    "assert X_transformed.shape[1] == len(fixed_feature_names), (\n",
    "    \"Number of feature names (%d) != number of columns in array (%d)\" \n",
    "    % (len(fixed_feature_names), X_transformed.shape[1])\n",
    ")\n",
    "\n",
    "# 6. Build a DataFrame for easy inspection\n",
    "processed_df = pd.DataFrame(\n",
    "    X_transformed, \n",
    "    columns=fixed_feature_names, \n",
    "    index=df_raw.index\n",
    ")\n",
    "\n",
    "processed_df.head()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-02T21:21:37.324128900Z",
     "start_time": "2025-06-02T21:21:35.046240400Z"
    }
   },
   "id": "3a823090cd3b383"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "#Doing a log transformation while still distinguishing losses vs. gains.\n",
    "train['Net_Investment'] = train['Investment_Gain'] - train['Investment_Loss']\n",
    "test['Net_Investment'] = test['Investment_Gain'] - test['Investment_Loss']\n",
    "\n",
    "train['Log_Net_Investment'] = np.sign(train['Net_Investment']) * np.log2(abs(train['Net_Investment']) + 1)\n",
    "test['Log_Net_Investment'] = np.sign(test['Net_Investment']) * np.log2(abs(test['Net_Investment']) + 1)\n",
    "\n",
    "\n",
    "# Side-by-side boxplots \n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "sns.boxplot(x=train['Net_Investment'], ax=axes[0])\n",
    "axes[0].set_title('Original Net_Investment')\n",
    "axes[0].set_xlabel('Net_Investment')\n",
    "\n",
    "sns.boxplot(x=train['Log_Net_Investment'], ax=axes[1])\n",
    "axes[1].set_title('Log-Transformed Net_Investment')\n",
    "axes[1].set_xlabel('Log_Net_Investment')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Side-by-side histograms\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "sns.histplot(train['Net_Investment'].dropna(), kde=True, ax=axes[0])\n",
    "axes[0].set_title('Distribution of Net_Investment')\n",
    "axes[0].set_xlabel('Net_Investment')\n",
    "\n",
    "sns.histplot(train['Log_Net_Investment'].dropna(), kde=True, ax=axes[1])\n",
    "axes[1].set_title('Distribution of Log-Transformed Net_Investment')\n",
    "axes[1].set_xlabel('Log_Net_Investment')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-01T12:48:38.159956400Z",
     "start_time": "2025-06-01T12:48:38.095544100Z"
    }
   },
   "id": "572f1caebdf1dfa5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train['log_gain'] = np.log2(train['Investment_Gain'] + 1)\n",
    "train['log_loss'] = np.log2(train['Investment_Loss'] + 1)\n",
    "train['net_investment_log'] = train['log_gain'] - train['log_loss']\n",
    "\n",
    "plt.hist(train['net_investment_log'])\n",
    "plt.show()\n",
    "\n",
    "sns.histplot(train['net_investment_log'].dropna(), kde=True)\n",
    "plt.title(f\"Distribution of net_investment_log\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-06-01T12:48:38.098090800Z"
    }
   },
   "id": "bc94fa2c0e4a4e18"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We now have a few ways of dealing with the investments: \n",
    "1. keeping each one in the original way.\n",
    "2. keeping each one with a log transformation.\n",
    "3. Log on each one and then difference\n",
    "4. Difference and then log on each one.\n",
    "\n",
    "we will need to do some cross-validation and check which one gives the best results."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c4e7a8147e8d9782"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We applied a log transformation to Net_Investment to reduce skew and compress extreme outliers.\n",
    "The resulting distribution is more symmetric, so we will keep it.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dcc19d2573d9a90b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We applied a log transformation to column A to reduce skew and potentially improve its relationship with the target.\n",
    "However, the correlation with Loan_Approval remained low, so we won't use it."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dcaf1ff96686fb72"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pd.crosstab(train['Loan_Approval'], train['Gender'])\n",
    "pd.crosstab(train['Loan_Approval'], train['Ethnicity'], normalize='index')\n",
    "pd.crosstab(train['Loan_Approval'], train['Education_Level'], normalize='index')\n",
    "pd.crosstab(train['Loan_Approval_str'], train['Gender'], normalize='index')\n",
    "\n",
    "# Doesn't really help because of the unbalanced proportions. Add an explanation.\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-06-01T12:48:38.099523Z"
    }
   },
   "id": "ecc695970f3c9fe"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# AFTER CHANGING \n",
    "\n",
    "# Checking which variables are most correlated with Loan_approval (only linear):\n",
    "numeric_df = train.select_dtypes(include='number')\n",
    "correlations = numeric_df.corr()['Loan_Approval'].sort_values(ascending=False)\n",
    "print(correlations)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-06-01T12:48:38.101531600Z"
    }
   },
   "id": "1d1ac20a311400d4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Data Preprocessing\n",
    "_Handle missing values, encode categorical features, normalize, etc._\n",
    "\n",
    "**✏️ Answer in markdown:**\n",
    "- Did you normalize the data? Why?\n",
    "- How did you handle categorical variables?\n",
    "- Did you apply PCA or feature selection?\n",
    "- Did you drop or create new features?\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aa6ac0592772b405"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "######## TRY AGAIN AFTER ADDING AGE\n",
    "'''\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "\n",
    "df = train  \n",
    "\n",
    "# 1. Define features & split known vs missing\n",
    "features = ['Weekly_Work_Hours', 'Years_of_Education', 'Age']\n",
    "known   = df[df['Investment_Gain'].notnull() ].copy()\n",
    "missing = df[df['Investment_Gain'].isnull()].copy()\n",
    "\n",
    "X_known = known[features]\n",
    "y_known = known['Investment_Gain']\n",
    "\n",
    "# 2. Hold out 20% for validation\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    X_known, y_known,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 3. Train on the 80%\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_tr, y_tr)\n",
    "\n",
    "# 4. Predict on the hold‐out and evaluate\n",
    "y_pred = lr.predict(X_val)\n",
    "print(f\"R² on hold-out: {r2_score(y_val, y_pred):.3f}\")\n",
    "print(f\"MAE on hold-out: {mean_absolute_error(y_val, y_pred):.3f}\")\n",
    "\n",
    "# 5. (Optional) Retrain on ALL known data and impute missings\n",
    "lr_full = LinearRegression()\n",
    "lr_full.fit(X_known, y_known)\n",
    "\n",
    "X_missing = missing[features]\n",
    "df.loc[df['Investment_Gain'].isnull(), 'Investment_Gain'] = lr_full.predict(X_missing)\n",
    "'''\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-06-01T12:48:38.104579300Z"
    }
   },
   "id": "582f303fb88677b8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We tried to handle the missing values with linear regression, but R^2 was 0.02, so we just impute with the median."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1bb220032e4c5488"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The plot shows that column B is completely empty, so we should remove it. The rest of the columns have some missing values, but there doesn’t seem to be any clear pattern."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bfde1cd4ed56b68c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 1. Create a boolean mask for missingness - job type\n",
    "mask_Job_Type = train['Job_Type'].isna()            # True where missing\n",
    "train['MissingFlag_Job_Type'] = mask_Job_Type.map({True: 'Missing', False: 'Present'})\n",
    "\n",
    "# 2. Summary statistics for numeric vars\n",
    "numeric_cols = ['Log_Net_Investment']\n",
    "#numeric_cols = ['Age', 'Years_of_Education', 'Weekly_Work_Hours', 'Net_Investment', 'A', 'D', 'E']\n",
    "print(train.groupby('MissingFlag_Job_Type')[numeric_cols].describe().unstack(1))\n",
    "\n",
    "''' bring back after - just sick of the big graphs.\n",
    "# 4. Frequencies for other categorical vars\n",
    "cat_cols = ['Marital_Status', 'Household_Role', ]\n",
    "for col in cat_cols:\n",
    "    freq = pd.crosstab(train['MissingFlag_Job_Type'], train[col], normalize='index')\n",
    "    print(f'\\nRelative frequency of {col}:\\n', freq)\n",
    "    freq.plot.bar(stacked=True)\n",
    "    plt.title(f'{col} by missingness')\n",
    "    plt.ylabel('Proportion')\n",
    "    plt.show()\n",
    "\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-06-01T12:48:38.107918Z"
    }
   },
   "id": "90178773caacf0f3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 1. Create a boolean mask for missingness with Investment_Gain to see if we can delete it to use YJ\n",
    "mask_Investment_Gain = train['Investment_Gain'].isna()            # True where missing\n",
    "train['MissingFlag_Investment_Gain'] = mask_Investment_Gain.map({True: 'Missing', False: 'Present'})\n",
    "\n",
    "# 2. Summary statistics for numeric vars\n",
    "numeric_cols = ['E']\n",
    "#numeric_cols = ['Age', 'Years_of_Education', 'Weekly_Work_Hours', 'A', 'D', 'E']\n",
    "print(train.groupby('MissingFlag_Investment_Gain')[numeric_cols].describe().unstack(1))\n",
    "\n",
    "\n",
    "''' bring back after - just sick of the big graphs.\n",
    "\n",
    "# 4. Frequencies for other categorical vars\n",
    "cat_cols = ['Marital_Status', 'Household_Role', ]\n",
    "for col in cat_cols:\n",
    "    freq = pd.crosstab(train['MissingFlag_Investment_Gain'], train[col], normalize='index')\n",
    "    print(f'\\nRelative frequency of {col}:\\n', freq)\n",
    "    freq.plot.bar(stacked=True)\n",
    "    plt.title(f'{col} by missingness')\n",
    "    plt.ylabel('Proportion')\n",
    "    plt.show()\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-06-01T12:48:38.109077800Z"
    }
   },
   "id": "4ec05495035e859f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 1. Create a boolean mask for missingness with Ethnicity\n",
    "mask_Ethnicity = train['Ethnicity'].isna()            # True where missing\n",
    "train['MissingFlag_Ethnicity'] = mask_Ethnicity.map({True: 'Missing', False: 'Present'})\n",
    "\n",
    "# 2. Summary statistics for numeric vars\n",
    "numeric_cols = ['D']\n",
    "#numeric_cols = ['Age', 'Years_of_Education', 'Weekly_Work_Hours', 'A', 'D', 'E']\n",
    "print(train.groupby('MissingFlag_Ethnicity')[numeric_cols].describe().unstack(1))\n",
    "\n",
    "'''\n",
    "#bring back after - just sick of the big graphs.\n",
    "\n",
    "# 4. Frequencies for other categorical vars\n",
    "cat_cols = ['Marital_Status', 'Household_Role', 'Employment_Type', 'Job_Type', 'Gender', 'Country_of_Residence', 'Preferred_Communication_Method']\n",
    "for col in cat_cols:\n",
    "    freq = pd.crosstab(train['MissingFlag_Ethnicity'], train[col], normalize='index')\n",
    "    print(f'\\nRelative frequency of {col}:\\n', freq)\n",
    "    freq.plot.bar(stacked=True)\n",
    "    plt.title(f'{col} by missingness')\n",
    "    plt.ylabel('Proportion')\n",
    "    plt.show()\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-06-01T12:48:38.111104800Z"
    }
   },
   "id": "8bffe75d52c78054"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 1. Create a boolean mask for missingness with Ethnicity\n",
    "mask_Ethnicity = train['Preferred_Communication_Method'].isna()            # True where missing\n",
    "train['MissingFlag_Ethnicity'] = mask_Ethnicity.map({True: 'Missing', False: 'Present'})\n",
    "\n",
    "# 2. Summary statistics for numeric vars\n",
    "numeric_cols = ['Weekly_Work_Hours']\n",
    "#numeric_cols = ['Age', 'Years_of_Education', 'Weekly_Work_Hours', 'A', 'D', 'E']\n",
    "print(train.groupby('MissingFlag_Ethnicity')[numeric_cols].describe().unstack(1))\n",
    "\n",
    "'''\n",
    "#bring back after - just sick of the big graphs.\n",
    "\n",
    "# 4. Frequencies for other categorical vars\n",
    "cat_cols = ['Marital_Status', 'Household_Role', 'Employment_Type', 'Job_Type', 'Gender', 'Country_of_Residence', 'Preferred_Communication_Method']\n",
    "for col in cat_cols:\n",
    "    freq = pd.crosstab(train['MissingFlag_Ethnicity'], train[col], normalize='index')\n",
    "    print(f'\\nRelative frequency of {col}:\\n', freq)\n",
    "    freq.plot.bar(stacked=True)\n",
    "    plt.title(f'{col} by missingness')\n",
    "    plt.ylabel('Proportion')\n",
    "    plt.show()\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-06-01T12:48:38.112109400Z"
    }
   },
   "id": "ebcdeacd8c6f8288"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 1. Create a boolean mask for Job tyoe with Ethnicity\n",
    "mask_Ethnicity = train['Job_Type'].isna()            # True where missing\n",
    "train['MissingFlag_Ethnicity'] = mask_Ethnicity.map({True: 'Missing', False: 'Present'})\n",
    "\n",
    "# 2. Summary statistics for numeric vars\n",
    "numeric_cols = ['Weekly_Work_Hours']\n",
    "#numeric_cols = ['Age', 'Years_of_Education', 'Weekly_Work_Hours', 'A', 'D', 'E']\n",
    "print(train.groupby('MissingFlag_Ethnicity')[numeric_cols].describe().unstack(1))\n",
    "\n",
    "'''\n",
    "#bring back after - just sick of the big graphs.\n",
    "\n",
    "# 4. Frequencies for other categorical vars\n",
    "cat_cols = ['Marital_Status', 'Household_Role', 'Employment_Type', 'Job_Type', 'Gender', 'Country_of_Residence', 'Preferred_Communication_Method']\n",
    "for col in cat_cols:\n",
    "    freq = pd.crosstab(train['MissingFlag_Ethnicity'], train[col], normalize='index')\n",
    "    print(f'\\nRelative frequency of {col}:\\n', freq)\n",
    "    freq.plot.bar(stacked=True)\n",
    "    plt.title(f'{col} by missingness')\n",
    "    plt.ylabel('Proportion')\n",
    "    plt.show()\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-06-01T12:48:38.114495100Z"
    }
   },
   "id": "dc682854a7545428"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 1. Create a boolean mask for missingness with Ethnicity\n",
    "mask_Ethnicity = train['Employment_Type'].isna()            # True where missing\n",
    "train['MissingFlag_Ethnicity'] = mask_Ethnicity.map({True: 'Missing', False: 'Present'})\n",
    "\n",
    "# 2. Summary statistics for numeric vars\n",
    "numeric_cols = ['Weekly_Work_Hours']\n",
    "#numeric_cols = ['Age', 'Years_of_Education', 'Weekly_Work_Hours', 'A', 'D', 'E']\n",
    "print(train.groupby('MissingFlag_Ethnicity')[numeric_cols].describe().unstack(1))\n",
    "\n",
    "'''\n",
    "#bring back after - just sick of the big graphs.\n",
    "\n",
    "# 4. Frequencies for other categorical vars\n",
    "cat_cols = ['Marital_Status', 'Household_Role', 'Employment_Type', 'Job_Type', 'Gender', 'Country_of_Residence', 'Preferred_Communication_Method']\n",
    "for col in cat_cols:\n",
    "    freq = pd.crosstab(train['MissingFlag_Ethnicity'], train[col], normalize='index')\n",
    "    print(f'\\nRelative frequency of {col}:\\n', freq)\n",
    "    freq.plot.bar(stacked=True)\n",
    "    plt.title(f'{col} by missingness')\n",
    "    plt.ylabel('Proportion')\n",
    "    plt.show()\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-06-01T12:48:38.115570900Z"
    }
   },
   "id": "2f9d561e22c26d01"
  },
  {
   "cell_type": "markdown",
   "source": [
    "i see that the missing and present have a really different distribution. so i used knn"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "904a6fcd72dab2c4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We flagged missing Investment_Gain and saw identical “Missing” vs. “Present” distributions, so it’s MCAR. We will drop those rows to be able to apply Yeo–Johnson to the rest.- DONT THINK SO"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "edac95e97289ad40"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "\n",
    "a = train.loc[train['MissingFlag_Job_Type']=='Missing', 'Weekly_Work_Hours'].dropna()\n",
    "b = train.loc[train['MissingFlag_Job_Type']=='Present','Weekly_Work_Hours'].dropna()\n",
    "t_stat, p_val = ttest_ind(a, b, equal_var=False)\n",
    "print(f'p-value = {p_val}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-06-01T12:48:38.118046700Z"
    }
   },
   "id": "d43b1ed5929daff4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#notes:\n",
    "C is handeled with mean bacause its distributed normally."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "79660c63ed10f013"
  },
  {
   "cell_type": "markdown",
   "source": [
    "for values with more than 5 percent of missing data we checked if the data is missed randomly or not.\n",
    "we can see with the t-test that the missing job_type are not random. so we will use a more sofisticated way to deal with the missing values"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f136a7aed9a9fb2c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Training – Basic Models\n",
    "_Start simple. Train Logistic Regression or KNN._"
   ],
   "id": "85618933fc93ce16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-06-01T12:48:38.120068300Z"
    }
   },
   "id": "1b9e4e1a7e53ba54"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Training – Advanced Models\n",
    "Choose 3 models from:\n",
    "- Decision Tree\n",
    "- Random Forest or AdaBoost\n",
    "- Support Vector Machine\n",
    "- Multi-Layer Perceptron (Neural Net)\n",
    "\n",
    "_Include basic hyperparameter tuning or explain defaults._"
   ],
   "id": "ecc8b1eefecc948c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation\n",
    "- Build confusion matrix for at least one model\n",
    "- Generate ROC curves (using K-Fold CV)\n",
    "- Report AUC\n",
    "\n",
    "**✏️ Answer in markdown:**\n",
    "- Does any model overfit?\n",
    "- Which model generalizes best?\n",
    "- Final choice justification"
   ],
   "id": "65b470cc0d11e7b7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Final Prediction on Test Set\n",
    "_Predict probabilities (not binary labels!) and create submission file._"
   ],
   "id": "1f21f590332511f3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-06-01T12:48:38.121072800Z"
    }
   },
   "outputs": [],
   "source": [
    "# Example:\n",
    "# final_model.predict_proba(test)[:, 1]  # ← probability of 'approved'\n",
    "# submission = pd.DataFrame({\n",
    "#     'customer_id': test['customer_id'],\n",
    "#     'loan_approval': predictions\n",
    "# })\n",
    "# submission.to_csv('Submission_group_32.csv', index=False)"
   ],
   "id": "625f5bbc3680f72a"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Appendix / Extra Notes\n",
    "_Use this area for extra analysis, failed attempts, or ideas you explored._"
   ],
   "id": "ae46b3996890ae43"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
