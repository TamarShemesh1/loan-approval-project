{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project – Introduction to Machine Learning (Group 32)\n",
    "## Loan Approval Prediction – Binary Classification\n",
    "**Tamar & Tala – Spring 2025**\n",
    "\n",
    "This notebook walks through our end-to-end machine learning project:\n",
    "- Data exploration \n",
    "- Feature processing \n",
    "- Model training \n",
    "- Evaluation \n",
    "- Final prediction submission "
   ],
   "id": "382a8f2ce576c407"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Libraries and Data\n",
    "_We’ll start by loading the data and setting up useful libraries._"
   ],
   "id": "809af347f383058e"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T10:33:15.253566500Z",
     "start_time": "2025-05-30T10:33:13.311426400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   customer_id   Age Employment_Type       A Education_Level  \\\n0       115892  90.0             NaN   77053         HS-grad   \n1       115893  82.0         Private  132870         HS-grad   \n2       115895  54.0         Private  140359         7th-8th   \n3       115896  41.0         Private  264663    Some-college   \n4       115897  34.0         Private  216864         HS-grad   \n\n   Years_of_Education Marital_Status           Job_Type Household_Role  \\\n0                 9.0        Widowed                NaN  Not-in-family   \n1                 9.0        Widowed    Exec-managerial  Not-in-family   \n2                 4.0       Divorced  Machine-op-inspct      Unmarried   \n3                10.0      Separated     Prof-specialty      Own-child   \n4                 9.0       Divorced      Other-service      Unmarried   \n\n  Ethnicity  ... Investment_Gain  Investment_Loss  Weekly_Work_Hours  \\\n0     White  ...             0.0           4356.0                 40   \n1     White  ...             0.0           4356.0                 18   \n2     White  ...             0.0           3900.0                 40   \n3     White  ...             0.0           3900.0                 40   \n4     White  ...             NaN           3770.0                 45   \n\n   Country_of_Residence           C  Preferred_Communication_Method         D  \\\n0         United-States  104.906221                      Phone_Call  2.865629   \n1         United-States   96.358501                            mail  5.528583   \n2         United-States  115.529631                            mail  3.816915   \n3         United-States   85.732506                             NaN  5.416363   \n4         United-States  115.218443                            Mail  6.453932   \n\n    B           E  Loan_Approval  \n0 NaN  170.887465              0  \n1 NaN  156.630201              0  \n2 NaN  165.635557              0  \n3 NaN  123.379007              0  \n4 NaN  155.262131              0  \n\n[5 rows x 21 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>customer_id</th>\n      <th>Age</th>\n      <th>Employment_Type</th>\n      <th>A</th>\n      <th>Education_Level</th>\n      <th>Years_of_Education</th>\n      <th>Marital_Status</th>\n      <th>Job_Type</th>\n      <th>Household_Role</th>\n      <th>Ethnicity</th>\n      <th>...</th>\n      <th>Investment_Gain</th>\n      <th>Investment_Loss</th>\n      <th>Weekly_Work_Hours</th>\n      <th>Country_of_Residence</th>\n      <th>C</th>\n      <th>Preferred_Communication_Method</th>\n      <th>D</th>\n      <th>B</th>\n      <th>E</th>\n      <th>Loan_Approval</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>115892</td>\n      <td>90.0</td>\n      <td>NaN</td>\n      <td>77053</td>\n      <td>HS-grad</td>\n      <td>9.0</td>\n      <td>Widowed</td>\n      <td>NaN</td>\n      <td>Not-in-family</td>\n      <td>White</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>4356.0</td>\n      <td>40</td>\n      <td>United-States</td>\n      <td>104.906221</td>\n      <td>Phone_Call</td>\n      <td>2.865629</td>\n      <td>NaN</td>\n      <td>170.887465</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>115893</td>\n      <td>82.0</td>\n      <td>Private</td>\n      <td>132870</td>\n      <td>HS-grad</td>\n      <td>9.0</td>\n      <td>Widowed</td>\n      <td>Exec-managerial</td>\n      <td>Not-in-family</td>\n      <td>White</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>4356.0</td>\n      <td>18</td>\n      <td>United-States</td>\n      <td>96.358501</td>\n      <td>mail</td>\n      <td>5.528583</td>\n      <td>NaN</td>\n      <td>156.630201</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>115895</td>\n      <td>54.0</td>\n      <td>Private</td>\n      <td>140359</td>\n      <td>7th-8th</td>\n      <td>4.0</td>\n      <td>Divorced</td>\n      <td>Machine-op-inspct</td>\n      <td>Unmarried</td>\n      <td>White</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>3900.0</td>\n      <td>40</td>\n      <td>United-States</td>\n      <td>115.529631</td>\n      <td>mail</td>\n      <td>3.816915</td>\n      <td>NaN</td>\n      <td>165.635557</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>115896</td>\n      <td>41.0</td>\n      <td>Private</td>\n      <td>264663</td>\n      <td>Some-college</td>\n      <td>10.0</td>\n      <td>Separated</td>\n      <td>Prof-specialty</td>\n      <td>Own-child</td>\n      <td>White</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>3900.0</td>\n      <td>40</td>\n      <td>United-States</td>\n      <td>85.732506</td>\n      <td>NaN</td>\n      <td>5.416363</td>\n      <td>NaN</td>\n      <td>123.379007</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>115897</td>\n      <td>34.0</td>\n      <td>Private</td>\n      <td>216864</td>\n      <td>HS-grad</td>\n      <td>9.0</td>\n      <td>Divorced</td>\n      <td>Other-service</td>\n      <td>Unmarried</td>\n      <td>White</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>3770.0</td>\n      <td>45</td>\n      <td>United-States</td>\n      <td>115.218443</td>\n      <td>Mail</td>\n      <td>6.453932</td>\n      <td>NaN</td>\n      <td>155.262131</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 21 columns</p>\n</div>"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve, classification_report\n",
    "\n",
    "import missingno as msno\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "\n",
    "\n",
    "# Load the data\n",
    "train = pd.read_csv('train.csv', na_values=[\"?\"])\n",
    "test = pd.read_csv('test.csv', na_values=[\"?\"])\n",
    "train.head()"
   ],
   "id": "306e676c18666e27"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis (EDA)\n",
    "_Let's understand the data structure, spot missing values, and explore correlations._\n",
    "\n",
    "\n",
    "**Answer in markdown:**\n",
    "- Are there outliers?\n",
    "- Are there missing values?\n",
    "- Any early ideas on important features?"
   ],
   "id": "1fafd0a08e17e5a7"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "(27676, 21)"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_rows,n_columns = train.shape\n",
    "(n_rows,n_columns)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-30T10:33:15.321502600Z",
     "start_time": "2025-05-30T10:33:15.250573900Z"
    }
   },
   "id": "511a2b2c7c2410bf"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "         customer_id           Age             A  Years_of_Education  \\\ncount   27676.000000  27541.000000  2.767600e+04        27122.000000   \nmean   132196.289854     38.597727  1.898849e+05           10.079972   \nstd      9399.839425     13.636965  1.053593e+05            2.575332   \nmin    115892.000000     17.000000  1.228500e+04            1.000000   \n25%    124043.750000     28.000000  1.177652e+05            9.000000   \n50%    132222.500000     37.000000  1.785870e+05           10.000000   \n75%    140322.500000     48.000000  2.376340e+05           12.000000   \nmax    148452.000000     90.000000  1.484705e+06           16.000000   \n\n       Investment_Gain  Investment_Loss  Weekly_Work_Hours             C  \\\ncount     26846.000000     27647.000000       27676.000000  26561.000000   \nmean       1091.680101        86.084458          40.482151     99.957601   \nstd        7482.292675       400.504134          12.371156      9.911054   \nmin           0.000000         0.000000           1.000000     58.136513   \n25%           0.000000         0.000000          40.000000     93.273986   \n50%           0.000000         0.000000          40.000000     99.903569   \n75%           0.000000         0.000000          45.000000    106.636563   \nmax       99999.000000      4356.000000          99.000000    144.309842   \n\n                  D    B             E  Loan_Approval  \ncount  27676.000000  0.0  24389.000000   27676.000000  \nmean       4.987450  NaN    139.251615       0.240786  \nstd        1.511751  NaN     13.708225       0.427569  \nmin       -1.711896  NaN     82.675954       0.000000  \n25%        3.968762  NaN    129.783073       0.000000  \n50%        4.994815  NaN    139.078247       0.000000  \n75%        5.999440  NaN    148.488675       0.000000  \nmax       10.876204  NaN    199.134411       1.000000  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>customer_id</th>\n      <th>Age</th>\n      <th>A</th>\n      <th>Years_of_Education</th>\n      <th>Investment_Gain</th>\n      <th>Investment_Loss</th>\n      <th>Weekly_Work_Hours</th>\n      <th>C</th>\n      <th>D</th>\n      <th>B</th>\n      <th>E</th>\n      <th>Loan_Approval</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>27676.000000</td>\n      <td>27541.000000</td>\n      <td>2.767600e+04</td>\n      <td>27122.000000</td>\n      <td>26846.000000</td>\n      <td>27647.000000</td>\n      <td>27676.000000</td>\n      <td>26561.000000</td>\n      <td>27676.000000</td>\n      <td>0.0</td>\n      <td>24389.000000</td>\n      <td>27676.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>132196.289854</td>\n      <td>38.597727</td>\n      <td>1.898849e+05</td>\n      <td>10.079972</td>\n      <td>1091.680101</td>\n      <td>86.084458</td>\n      <td>40.482151</td>\n      <td>99.957601</td>\n      <td>4.987450</td>\n      <td>NaN</td>\n      <td>139.251615</td>\n      <td>0.240786</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>9399.839425</td>\n      <td>13.636965</td>\n      <td>1.053593e+05</td>\n      <td>2.575332</td>\n      <td>7482.292675</td>\n      <td>400.504134</td>\n      <td>12.371156</td>\n      <td>9.911054</td>\n      <td>1.511751</td>\n      <td>NaN</td>\n      <td>13.708225</td>\n      <td>0.427569</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>115892.000000</td>\n      <td>17.000000</td>\n      <td>1.228500e+04</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>58.136513</td>\n      <td>-1.711896</td>\n      <td>NaN</td>\n      <td>82.675954</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>124043.750000</td>\n      <td>28.000000</td>\n      <td>1.177652e+05</td>\n      <td>9.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>40.000000</td>\n      <td>93.273986</td>\n      <td>3.968762</td>\n      <td>NaN</td>\n      <td>129.783073</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>132222.500000</td>\n      <td>37.000000</td>\n      <td>1.785870e+05</td>\n      <td>10.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>40.000000</td>\n      <td>99.903569</td>\n      <td>4.994815</td>\n      <td>NaN</td>\n      <td>139.078247</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>140322.500000</td>\n      <td>48.000000</td>\n      <td>2.376340e+05</td>\n      <td>12.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>45.000000</td>\n      <td>106.636563</td>\n      <td>5.999440</td>\n      <td>NaN</td>\n      <td>148.488675</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>148452.000000</td>\n      <td>90.000000</td>\n      <td>1.484705e+06</td>\n      <td>16.000000</td>\n      <td>99999.000000</td>\n      <td>4356.000000</td>\n      <td>99.000000</td>\n      <td>144.309842</td>\n      <td>10.876204</td>\n      <td>NaN</td>\n      <td>199.134411</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-30T10:33:15.409680800Z",
     "start_time": "2025-05-30T10:33:15.266672700Z"
    }
   },
   "id": "92e56be4784cad8a"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 27676 entries, 0 to 27675\n",
      "Data columns (total 21 columns):\n",
      " #   Column                          Non-Null Count  Dtype  \n",
      "---  ------                          --------------  -----  \n",
      " 0   customer_id                     27676 non-null  int64  \n",
      " 1   Age                             27541 non-null  float64\n",
      " 2   Employment_Type                 25865 non-null  object \n",
      " 3   A                               27676 non-null  int64  \n",
      " 4   Education_Level                 27676 non-null  object \n",
      " 5   Years_of_Education              27122 non-null  float64\n",
      " 6   Marital_Status                  27676 non-null  object \n",
      " 7   Job_Type                        24305 non-null  object \n",
      " 8   Household_Role                  27676 non-null  object \n",
      " 9   Ethnicity                       24912 non-null  object \n",
      " 10  Gender                          27676 non-null  object \n",
      " 11  Investment_Gain                 26846 non-null  float64\n",
      " 12  Investment_Loss                 27647 non-null  float64\n",
      " 13  Weekly_Work_Hours               27676 non-null  int64  \n",
      " 14  Country_of_Residence            27180 non-null  object \n",
      " 15  C                               26561 non-null  float64\n",
      " 16  Preferred_Communication_Method  26812 non-null  object \n",
      " 17  D                               27676 non-null  float64\n",
      " 18  B                               0 non-null      float64\n",
      " 19  E                               24389 non-null  float64\n",
      " 20  Loan_Approval                   27676 non-null  int64  \n",
      "dtypes: float64(8), int64(4), object(9)\n",
      "memory usage: 4.4+ MB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-30T10:33:15.491739200Z",
     "start_time": "2025-05-30T10:33:15.326055800Z"
    }
   },
   "id": "6bd74af3cb16574"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Column B is completely empty (100% missing), so we will remove it from the dataset.\n",
    "Several columns, such as Employment_Type, Job_Type and Ethnicity have a high percentage of missing values.\n",
    "We'll need to decide whether to fill them or drop them."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9f58dd8d06540027"
  },
  {
   "cell_type": "markdown",
   "source": [
    "_Visualizing the variables to check their distributions:_"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "49bbe0e7870b3c7e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Histograms and count plots of each explanatory variable\n",
    "\n",
    "for col in train.columns:\n",
    "    plt.figure(figsize=(15, 4))\n",
    "    \n",
    "    # For numeric variables\n",
    "    if pd.api.types.is_numeric_dtype(train[col]):\n",
    "        sns.histplot(train[col].dropna(), kde=True)\n",
    "        plt.title(f\"Distribution of {col}\")\n",
    "    \n",
    "    # For categorical variables \n",
    "    else:\n",
    "        sns.countplot(x=col, data=train, order=train[col].value_counts().index)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.title(f\"Countplot of {col}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2f8d6bf1fc5b57f0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Bar plots of approval rates for categorical variables\n",
    "\n",
    "categorical_vars = [\n",
    "    'Employment_Type', 'Education_Level', 'Marital_Status', 'Job_Type',\n",
    "    'Household_Role', 'Ethnicity', 'Gender', 'Country_of_Residence',\n",
    "    'Preferred_Communication_Method'\n",
    "]\n",
    "\n",
    "for col in categorical_vars:\n",
    "    plt.figure(figsize=(15, 4))\n",
    "    sns.barplot(x=col, y='Loan_Approval', data=train, estimator='mean', order=train[col].value_counts().index)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.title(f'Approval Rate by {col}')\n",
    "    plt.ylabel('Approval Rate (Mean of Loan_approval)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "42a31bcdb5f5d849"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Box plots of approved vs. rejected for numerical variables\n",
    "train['Loan_Approval_str'] = train['Loan_Approval'].map({0: 'Rejected', 1: 'Approved'})\n",
    "\n",
    "# Remove columns that shouldn't be plotted\n",
    "numeric_cols = [col for col in train.select_dtypes(include='number').columns if col not in ['customer_id', 'Loan_Approval']]\n",
    "\n",
    "# Create one boxplot per feature vs Loan Approval\n",
    "for col in numeric_cols:\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.boxplot(x='Loan_Approval_str', y=col, data=train)\n",
    "    plt.title(f'{col} by Loan Approval')\n",
    "    plt.xlabel('Loan Approval')\n",
    "    plt.ylabel(col)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "343249a9cee851ed"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Key Insights from Feature Distributions**\n",
    "1. Employment_Type, Country_of_Residence and Ethnicity are highly imbalanced.\n",
    "To reduce sparsity and improve generalization, we will group the less frequent categories into broader, more meaningful groups.\n",
    "2. A is right-skewed, so a log transformation may help normalize its distribution.\n",
    "3. Investment_Gain and Investment_Loss are extremely skewed with large outliers, so a log transformation might help, and merging them into a single Net_Investment feature may provide a clearer signal.\n",
    "4. Preferred_Communication_Method has inconsistent values due to variations in capitalization and wording (e.g., \"Email\" vs \"email\"), so we unified similar values.\n",
    "5. Based on the approval rate bar plot for Household_Role, we observed clear patterns across the categories.\n",
    "We grouped them into three broader categories with similar approval behavior to simplify the feature and improve model stability.\n",
    "6. Column B is completely empty, so we decided to remove it.\n",
    "7.  We simplified the Country_of_Residence feature by grouping it into three categories: USA, Mexico, and Other. The vast majority of people were from the United States, and the approval rate among other countries wasn't very different.\n",
    "8.  We simplified the Employment_Type feature by grouping it into four categories: Private, not private and, and non-working.  The vast majority of people were private, and the noticeable difference in the approval rate was between non-private to non-working.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9e012bc51fce0e55"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Creating a heatmap\n",
    "\n",
    "# Select only numeric columns\n",
    "numeric_df = train.select_dtypes(include='number')\n",
    "\n",
    "# Compute the correlation matrix\n",
    "correlation_matrix = numeric_df.corr()\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", square=True)\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-05-30T10:33:35.228035900Z"
    }
   },
   "id": "4198104465d53ee8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Checking which variables are most correlated with Loan_approval (only linear) - this is the Loan_Approval row in the heatmap\n",
    "numeric_df = train.select_dtypes(include='number')\n",
    "correlations = numeric_df.corr()['Loan_Approval'].sort_values(ascending=False)\n",
    "print(correlations)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-05-30T10:33:35.230488200Z"
    }
   },
   "id": "dd0e681b69d85555"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Key Insights from the heat map**\n",
    "C and E are highly correlated (r = 0.87), indicating redundancy.\n",
    "Neither has a strong linear correlation with the target, but E shows a slightly stronger signal (r = 0.125). Even though E has more missing values (as we will see later on), it’s still more useful, so we’ll keep E and drop C. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "18833f3e4451f9f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We checked for overlap between Employment_Type and Job_Type using a cross-tabulation heatmap.\n",
    "The results show no strong or consistent relationship between the two — most employment types are associated with a wide variety of job types.\n",
    "Therefore, we concluded that the two features capture distinct information and chose to keep both."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dbba21d75f846898"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "crosstab = pd.crosstab(train['Employment_Type'], train['Job_Type'], normalize='index')\n",
    "plt.figure(figsize=(14, 7))\n",
    "sns.heatmap(crosstab, cmap=\"Blues\", annot=True, fmt=\".2f\")\n",
    "plt.title(\"Employment Type vs Job Type\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-05-30T10:33:35.233868300Z"
    }
   },
   "id": "2c2ad593ba0cad71"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Changes based on the key insights:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "114b5f5a38536405"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 1. Removing columns B and C\n",
    "train = train.drop(['B', 'C'], axis=1)\n",
    "test = test.drop(['B', 'C'], axis=1)\n",
    "n_columns -= 2"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e60c0335a6987d0f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 2. unifying similar values in Preferred_Communication_Method:\n",
    "train['Preferred_Communication_Method'] = train['Preferred_Communication_Method'].replace({\n",
    "    'mail': 'Mail',\n",
    "    'email': 'Email',\n",
    "    'phone': 'Phone',\n",
    "    'phone call': 'Phone'\n",
    "})\n",
    "test['Preferred_Communication_Method'] = test['Preferred_Communication_Method'].replace({\n",
    "    'mail': 'Mail',\n",
    "    'email': 'Email',\n",
    "    'phone': 'Phone',\n",
    "    'phone call': 'Phone'\n",
    "})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-05-30T10:33:35.245291700Z"
    }
   },
   "id": "7cc3d83646d50c09"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Grouping Country_of_Residence\n",
    "def group_country(country):\n",
    "    if country == \"United-States\":\n",
    "        return \"USA\"\n",
    "    elif country == \"Mexico\":\n",
    "        return \"Mexico\"\n",
    "    else:\n",
    "        return \"Other\"\n",
    "    \n",
    "train['Country_of_Residence'] = train['Country_of_Residence'].apply(group_country)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-05-30T10:33:35.249170800Z"
    }
   },
   "id": "ea59906540420095"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Grouping Education_Level\n",
    "def group_education(edu):\n",
    "    pre_school = ['Preschool']\n",
    "    school = ['1st-4th', '5th-6th', '9th', '10th', '11th', '12th', '7th-8th']\n",
    "    post_hs = ['HS-grad', 'Some-college']\n",
    "    assoc = ['Assoc-voc', 'Assoc-acdm']\n",
    "    higher = ['Masters', 'Bachelors']\n",
    "    postgraduate = ['Doctorate', 'Prof-school']\n",
    "    \n",
    "    if edu in pre_school:\n",
    "        return 'pre_school'\n",
    "    elif edu in school:\n",
    "        return 'school'\n",
    "    elif edu in post_hs:\n",
    "        return 'post_hs'\n",
    "    elif edu in assoc:\n",
    "        return 'assoc'\n",
    "    elif edu in higher:\n",
    "        return 'higher'\n",
    "    elif edu in postgraduate:\n",
    "        return 'postgraduate'\n",
    "    else:\n",
    "        return 'Other'\n",
    "    \n",
    "train['Education_Level'] = train['Education_Level'].apply(group_education)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-05-30T10:33:35.257291900Z"
    }
   },
   "id": "dc268012d6a3ebac"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def group_household_role(role):\n",
    "    if role in ['Husband', 'Wife']:\n",
    "        return 'Spouse'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "train['Household_Role'] = train['Household_Role'].apply(group_household_role)\n",
    "test['Household_Role'] = test['Household_Role'].apply(group_household_role)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-05-30T10:33:35.263663700Z"
    }
   },
   "id": "4c5ded80e2f52307"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def group_employment(emp):\n",
    "    if emp == 'Private':\n",
    "        return 'Private'\n",
    "    elif emp in ['Self-emp-not-inc', 'Self-emp-inc', 'Local-gov', 'State-gov', 'Federal-gov']:\n",
    "        return 'Not-Private'\n",
    "    elif emp in ['Without-pay', 'Never-worked']:\n",
    "        return 'Non-working'\n",
    "    else:\n",
    "        return 'Other'\n",
    "    \n",
    "    train['Employment_Grouped'] = train['Employment_Type'].apply(group_employment)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-30T10:33:35.310894500Z",
     "start_time": "2025-05-30T10:33:35.274438500Z"
    }
   },
   "id": "1d51f5ef046ab2cf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def group_ethnicity(eth):\n",
    "    if eth == 'White':\n",
    "        return 'White'\n",
    "    elif eth == 'Black':\n",
    "        return 'Black'\n",
    "    else:\n",
    "        return 'Other'\n",
    "        \n",
    "\n",
    "    train['Ethnicity'] = train['Ethnicity'].apply(group_ethnicity)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-05-30T10:33:35.281078700Z"
    }
   },
   "id": "3284ace8d6ae809d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a2d1e1d9da5d0134"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''\n",
    "'#Doing a log transformation while still distinguishing losses vs. gains.\n",
    "train['Net_Investment'] = train['Investment_Gain'] - train['Investment_Loss']\n",
    "test['Net_Investment'] = test['Investment_Gain'] - test['Investment_Loss']\n",
    "\n",
    "train['Log_Net_Investment'] = np.sign(train['Net_Investment']) * np.log2(abs(train['Net_Investment']) + 1)\n",
    "test['Log_Net_Investment'] = np.sign(test['Net_Investment']) * np.log2(abs(test['Net_Investment']) + 1)\n",
    "\n",
    "\n",
    "# Side-by-side boxplots \n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "sns.boxplot(x=train['Net_Investment'], ax=axes[0])\n",
    "axes[0].set_title('Original Net_Investment')\n",
    "axes[0].set_xlabel('Net_Investment')\n",
    "\n",
    "sns.boxplot(x=train['Log_Net_Investment'], ax=axes[1])\n",
    "axes[1].set_title('Log-Transformed Net_Investment')\n",
    "axes[1].set_xlabel('Log_Net_Investment')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Side-by-side histograms\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "sns.histplot(train['Net_Investment'].dropna(), kde=True, ax=axes[0])\n",
    "axes[0].set_title('Distribution of Net_Investment')\n",
    "axes[0].set_xlabel('Net_Investment')\n",
    "\n",
    "sns.histplot(train['Log_Net_Investment'].dropna(), kde=True, ax=axes[1])\n",
    "axes[1].set_title('Distribution of Log-Transformed Net_Investment')\n",
    "axes[1].set_xlabel('Log_Net_Investment')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-05-30T10:33:35.287530800Z"
    }
   },
   "id": "572f1caebdf1dfa5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train['log_gain'] = np.log2(train['Investment_Gain'] + 1)\n",
    "train['log_loss'] = np.log2(train['Investment_Loss'] + 1)\n",
    "train['net_investment_log'] = train['log_gain'] - train['log_loss']\n",
    "\n",
    "plt.hist(train['net_investment_log'])\n",
    "plt.show()\n",
    "\n",
    "sns.histplot(train['net_investment_log'].dropna(), kde=True)\n",
    "plt.title(f\"Distribution of net_investment_log\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-05-30T10:33:35.291926300Z"
    }
   },
   "id": "bc94fa2c0e4a4e18"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We now have a few ways of dealing with the investments: \n",
    "1. keeping each one in the original way.\n",
    "2. keeping each one with a log transformation.\n",
    "3. Log on each one and then difference\n",
    "4. Difference and then log on each one.\n",
    "\n",
    "we will need to do some cross-validation and check which one gives the best results."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c4e7a8147e8d9782"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We applied a log transformation to Net_Investment to reduce skew and compress extreme outliers.\n",
    "The resulting distribution is more symmetric, so we will keep it.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dcc19d2573d9a90b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We applied a log transformation to column A to reduce skew and potentially improve its relationship with the target.\n",
    "However, the correlation with Loan_Approval remained low, so we won't use it."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dcaf1ff96686fb72"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pd.crosstab(train['Loan_Approval'], train['Gender'])\n",
    "pd.crosstab(train['Loan_Approval'], train['Ethnicity'], normalize='index')\n",
    "pd.crosstab(train['Loan_Approval'], train['Education_Level'], normalize='index')\n",
    "pd.crosstab(train['Loan_Approval_str'], train['Gender'], normalize='index')\n",
    "\n",
    "# Doesn't really help because of the unbalanced proportions. Add an explanation.\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-05-30T10:33:35.294665400Z"
    }
   },
   "id": "ecc695970f3c9fe"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# AFTER CHANGING \n",
    "\n",
    "# Checking which variables are most correlated with Loan_approval (only linear):\n",
    "numeric_df = train.select_dtypes(include='number')\n",
    "correlations = numeric_df.corr()['Loan_Approval'].sort_values(ascending=False)\n",
    "print(correlations)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-05-30T10:33:35.296908500Z"
    }
   },
   "id": "1d1ac20a311400d4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Handling missing Years_Of_Education using the categorical Education_level\n",
    "# 1. Define the mapping (as in the UCI Adult dataset’s education-num)\n",
    "edu_to_years = {\n",
    "    'Preschool'   : 0,\n",
    "    '1st-4th'     : (1+2+3+4)/4,\n",
    "    '5th-6th'     : 5.5,\n",
    "    '7th-8th'     : 7.5,\n",
    "    '9th'         : 9,\n",
    "    '10th'        : 10,\n",
    "    '11th'        : 11,\n",
    "    '12th'        : 12,\n",
    "    'HS-grad'     : 12,\n",
    "    'Some-college': (12+13+14+15)/4,\n",
    "    'Assoc-voc'   : 14,\n",
    "    'Assoc-acdm'  : 14,\n",
    "    'Bachelors'   : 16,\n",
    "    'Masters'     : 18,\n",
    "    'Prof-school' : 19,  \n",
    "    'Doctorate'   : 20   \n",
    "}\n",
    "\n",
    "# 2. Fill missing based on that map\n",
    "for df in (train, test):\n",
    "    df['Years_of_Education'] = df['Years_of_Education'].fillna(\n",
    "        df['Education_Level'].map(edu_to_years)\n",
    "    )\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-05-30T10:33:35.299714700Z"
    }
   },
   "id": "274e4c44ca5f1522"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Data Preprocessing\n",
    "_Handle missing values, encode categorical features, normalize, etc._\n",
    "\n",
    "**✏️ Answer in markdown:**\n",
    "- Did you normalize the data? Why?\n",
    "- How did you handle categorical variables?\n",
    "- Did you apply PCA or feature selection?\n",
    "- Did you drop or create new features?\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aa6ac0592772b405"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Understanding missing values - Check how many missing values there are in each column and if it has a pattern\n",
    "missing = train.isnull().sum()\n",
    "missing_percent = (missing / len(train)) * 100\n",
    "\n",
    "print(missing_percent.sort_values(ascending=False))\n",
    "\n",
    "# Plot missing value matrix\n",
    "#msno.matrix(train)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-05-30T10:33:35.302343Z"
    }
   },
   "id": "ba71c390d874d7a7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "######## TRY AGAIN AFTER ADDING AGE\n",
    "'''\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "\n",
    "df = train  \n",
    "\n",
    "# 1. Define features & split known vs missing\n",
    "features = ['Weekly_Work_Hours', 'Years_of_Education', 'Age']\n",
    "known   = df[df['Investment_Gain'].notnull() ].copy()\n",
    "missing = df[df['Investment_Gain'].isnull()].copy()\n",
    "\n",
    "X_known = known[features]\n",
    "y_known = known['Investment_Gain']\n",
    "\n",
    "# 2. Hold out 20% for validation\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    X_known, y_known,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 3. Train on the 80%\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_tr, y_tr)\n",
    "\n",
    "# 4. Predict on the hold‐out and evaluate\n",
    "y_pred = lr.predict(X_val)\n",
    "print(f\"R² on hold-out: {r2_score(y_val, y_pred):.3f}\")\n",
    "print(f\"MAE on hold-out: {mean_absolute_error(y_val, y_pred):.3f}\")\n",
    "\n",
    "# 5. (Optional) Retrain on ALL known data and impute missings\n",
    "lr_full = LinearRegression()\n",
    "lr_full.fit(X_known, y_known)\n",
    "\n",
    "X_missing = missing[features]\n",
    "df.loc[df['Investment_Gain'].isnull(), 'Investment_Gain'] = lr_full.predict(X_missing)\n",
    "'''\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-05-30T10:33:35.304381100Z"
    }
   },
   "id": "582f303fb88677b8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We tried to handle the missing values with linear regression, but R^2 was 0.02, so we just impute with the median."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1bb220032e4c5488"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The plot shows that column B is completely empty, so we should remove it. The rest of the columns have some missing values, but there doesn’t seem to be any clear pattern."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bfde1cd4ed56b68c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# OPTION 1: put missing (better with stuff like trees:  ['Job_Type', 'Ethnicity']\n",
    "\n",
    "# Define the columns\n",
    "categorical_col = ['Job_Type', 'Ethnicity', 'Preferred_Communication_Method']\n",
    "categorical_col_majority = ['Country_Of_Residence', 'Ethnicity']\n",
    "numerical_col = ['E', 'C']\n",
    "skw_numerical_col = ['Age', 'Investment_gain', 'Investment_Loss']\n",
    "\n",
    "# Pipeline for categorical features\n",
    "cat_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='Missing')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Pipeline for categorical features with distinct majorities\n",
    "cat_maj_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "\n",
    "# Pipeline for numerical features (mean imputation)\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean'))\n",
    "])\n",
    "\n",
    "# Pipeline for numerical features (median imputation) - for skewed columns\n",
    "skw_numerical_col = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median'))\n",
    "])\n",
    "\n",
    "\n",
    "# Combine both pipelines into a ColumnTransformer\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('cat', cat_pipeline, [categorical_col]),\n",
    "    ('num', num_pipeline, [numerical_col]),\n",
    "    ('skw_num', skw_numerical_col, [skw_numerical_col]),\n",
    "    'cat_maj', cat_maj_pipeline, [categorical_col_majority]\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-05-30T10:33:35.306702100Z"
    }
   },
   "id": "cbb0bc327c888228"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 1. Create a boolean mask for missingness - job type\n",
    "mask_Job_Type = train['Job_Type'].isna()            # True where missing\n",
    "train['MissingFlag_Job_Type'] = mask_Job_Type.map({True: 'Missing', False: 'Present'})\n",
    "\n",
    "# 2. Summary statistics for numeric vars\n",
    "numeric_cols = ['Log_Net_Investment']\n",
    "#numeric_cols = ['Age', 'Years_of_Education', 'Weekly_Work_Hours', 'Net_Investment', 'A', 'D', 'E']\n",
    "print(train.groupby('MissingFlag_Job_Type')[numeric_cols].describe().unstack(1))\n",
    "\n",
    "''' bring back after - just sick of the big graphs.\n",
    "# 4. Frequencies for other categorical vars\n",
    "cat_cols = ['Marital_Status', 'Household_Role', ]\n",
    "for col in cat_cols:\n",
    "    freq = pd.crosstab(train['MissingFlag_Job_Type'], train[col], normalize='index')\n",
    "    print(f'\\nRelative frequency of {col}:\\n', freq)\n",
    "    freq.plot.bar(stacked=True)\n",
    "    plt.title(f'{col} by missingness')\n",
    "    plt.ylabel('Proportion')\n",
    "    plt.show()\n",
    "\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-30T10:33:35.360101100Z",
     "start_time": "2025-05-30T10:33:35.310894500Z"
    }
   },
   "id": "90178773caacf0f3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 1. Create a boolean mask for missingness with Investment_Gain to see if we can delete it to use YJ\n",
    "mask_Investment_Gain = train['Investment_Gain'].isna()            # True where missing\n",
    "train['MissingFlag_Investment_Gain'] = mask_Investment_Gain.map({True: 'Missing', False: 'Present'})\n",
    "\n",
    "# 2. Summary statistics for numeric vars\n",
    "numeric_cols = ['E']\n",
    "#numeric_cols = ['Age', 'Years_of_Education', 'Weekly_Work_Hours', 'A', 'D', 'E']\n",
    "print(train.groupby('MissingFlag_Investment_Gain')[numeric_cols].describe().unstack(1))\n",
    "\n",
    "\n",
    "''' bring back after - just sick of the big graphs.\n",
    "\n",
    "# 4. Frequencies for other categorical vars\n",
    "cat_cols = ['Marital_Status', 'Household_Role', ]\n",
    "for col in cat_cols:\n",
    "    freq = pd.crosstab(train['MissingFlag_Investment_Gain'], train[col], normalize='index')\n",
    "    print(f'\\nRelative frequency of {col}:\\n', freq)\n",
    "    freq.plot.bar(stacked=True)\n",
    "    plt.title(f'{col} by missingness')\n",
    "    plt.ylabel('Proportion')\n",
    "    plt.show()\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-05-30T10:33:35.312262300Z"
    }
   },
   "id": "4ec05495035e859f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 1. Create a boolean mask for missingness with Ethnicity\n",
    "mask_Ethnicity = train['Ethnicity'].isna()            # True where missing\n",
    "train['MissingFlag_Ethnicity'] = mask_Ethnicity.map({True: 'Missing', False: 'Present'})\n",
    "\n",
    "# 2. Summary statistics for numeric vars\n",
    "numeric_cols = ['D']\n",
    "#numeric_cols = ['Age', 'Years_of_Education', 'Weekly_Work_Hours', 'A', 'D', 'E']\n",
    "print(train.groupby('MissingFlag_Ethnicity')[numeric_cols].describe().unstack(1))\n",
    "\n",
    "'''\n",
    "#bring back after - just sick of the big graphs.\n",
    "\n",
    "# 4. Frequencies for other categorical vars\n",
    "cat_cols = ['Marital_Status', 'Household_Role', 'Employment_Type', 'Job_Type', 'Gender', 'Country_of_Residence', 'Preferred_Communication_Method']\n",
    "for col in cat_cols:\n",
    "    freq = pd.crosstab(train['MissingFlag_Ethnicity'], train[col], normalize='index')\n",
    "    print(f'\\nRelative frequency of {col}:\\n', freq)\n",
    "    freq.plot.bar(stacked=True)\n",
    "    plt.title(f'{col} by missingness')\n",
    "    plt.ylabel('Proportion')\n",
    "    plt.show()\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-05-30T10:33:35.314422300Z"
    }
   },
   "id": "8bffe75d52c78054"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 1. Create a boolean mask for missingness with Ethnicity\n",
    "mask_Ethnicity = train['Preferred_Communication_Method'].isna()            # True where missing\n",
    "train['MissingFlag_Ethnicity'] = mask_Ethnicity.map({True: 'Missing', False: 'Present'})\n",
    "\n",
    "# 2. Summary statistics for numeric vars\n",
    "numeric_cols = ['Weekly_Work_Hours']\n",
    "#numeric_cols = ['Age', 'Years_of_Education', 'Weekly_Work_Hours', 'A', 'D', 'E']\n",
    "print(train.groupby('MissingFlag_Ethnicity')[numeric_cols].describe().unstack(1))\n",
    "\n",
    "'''\n",
    "#bring back after - just sick of the big graphs.\n",
    "\n",
    "# 4. Frequencies for other categorical vars\n",
    "cat_cols = ['Marital_Status', 'Household_Role', 'Employment_Type', 'Job_Type', 'Gender', 'Country_of_Residence', 'Preferred_Communication_Method']\n",
    "for col in cat_cols:\n",
    "    freq = pd.crosstab(train['MissingFlag_Ethnicity'], train[col], normalize='index')\n",
    "    print(f'\\nRelative frequency of {col}:\\n', freq)\n",
    "    freq.plot.bar(stacked=True)\n",
    "    plt.title(f'{col} by missingness')\n",
    "    plt.ylabel('Proportion')\n",
    "    plt.show()\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-05-30T10:33:35.316416700Z"
    }
   },
   "id": "ebcdeacd8c6f8288"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 1. Create a boolean mask for Job tyoe with Ethnicity\n",
    "mask_Ethnicity = train['Job_Type'].isna()            # True where missing\n",
    "train['MissingFlag_Ethnicity'] = mask_Ethnicity.map({True: 'Missing', False: 'Present'})\n",
    "\n",
    "# 2. Summary statistics for numeric vars\n",
    "numeric_cols = ['Weekly_Work_Hours']\n",
    "#numeric_cols = ['Age', 'Years_of_Education', 'Weekly_Work_Hours', 'A', 'D', 'E']\n",
    "print(train.groupby('MissingFlag_Ethnicity')[numeric_cols].describe().unstack(1))\n",
    "\n",
    "'''\n",
    "#bring back after - just sick of the big graphs.\n",
    "\n",
    "# 4. Frequencies for other categorical vars\n",
    "cat_cols = ['Marital_Status', 'Household_Role', 'Employment_Type', 'Job_Type', 'Gender', 'Country_of_Residence', 'Preferred_Communication_Method']\n",
    "for col in cat_cols:\n",
    "    freq = pd.crosstab(train['MissingFlag_Ethnicity'], train[col], normalize='index')\n",
    "    print(f'\\nRelative frequency of {col}:\\n', freq)\n",
    "    freq.plot.bar(stacked=True)\n",
    "    plt.title(f'{col} by missingness')\n",
    "    plt.ylabel('Proportion')\n",
    "    plt.show()\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-05-30T10:33:35.318582300Z"
    }
   },
   "id": "dc682854a7545428"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 1. Create a boolean mask for missingness with Ethnicity\n",
    "mask_Ethnicity = train['Employment_Type'].isna()            # True where missing\n",
    "train['MissingFlag_Ethnicity'] = mask_Ethnicity.map({True: 'Missing', False: 'Present'})\n",
    "\n",
    "# 2. Summary statistics for numeric vars\n",
    "numeric_cols = ['Weekly_Work_Hours']\n",
    "#numeric_cols = ['Age', 'Years_of_Education', 'Weekly_Work_Hours', 'A', 'D', 'E']\n",
    "print(train.groupby('MissingFlag_Ethnicity')[numeric_cols].describe().unstack(1))\n",
    "\n",
    "'''\n",
    "#bring back after - just sick of the big graphs.\n",
    "\n",
    "# 4. Frequencies for other categorical vars\n",
    "cat_cols = ['Marital_Status', 'Household_Role', 'Employment_Type', 'Job_Type', 'Gender', 'Country_of_Residence', 'Preferred_Communication_Method']\n",
    "for col in cat_cols:\n",
    "    freq = pd.crosstab(train['MissingFlag_Ethnicity'], train[col], normalize='index')\n",
    "    print(f'\\nRelative frequency of {col}:\\n', freq)\n",
    "    freq.plot.bar(stacked=True)\n",
    "    plt.title(f'{col} by missingness')\n",
    "    plt.ylabel('Proportion')\n",
    "    plt.show()\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-05-30T10:33:35.321380700Z"
    }
   },
   "id": "2f9d561e22c26d01"
  },
  {
   "cell_type": "markdown",
   "source": [
    "i see that the missing and present have a really different distribution. so i used knn"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "904a6fcd72dab2c4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "num_cols = ['Age', 'Years_of_Education', 'Weekly_Work_Hours']          \n",
    "cat_cols = ['Job_Type', 'Marital_Status']          \n",
    "knn_col = 'Employment_Type'  \n",
    "\n",
    "# 1) Encode all categoricals (including the target column) into ordinal codes:\n",
    "#    We'll impute missing codes later, so give them a placeholder value.\n",
    "cat_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "\n",
    "# 2) KNN imputer\n",
    "knn_imputer = KNNImputer(n_neighbors=3, weights='uniform')\n",
    "\n",
    "# 3) Build a ColumnTransformer that\n",
    "#    - one-hot encodes all categoricals\n",
    "#    - scales numerics\n",
    "#    - then runs the KNN imputer on *all* columns\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('encode_cat', cat_encoder, cat_cols + [knn_col]),\n",
    "    ('scale_num', StandardScaler(), num_cols)\n",
    "])\n",
    "\n",
    "model_pipe = Pipeline([\n",
    "    ('preproc', preprocessor),\n",
    "    ('impute_knn', knn_imputer),\n",
    "    ('clf', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "# 4) Validate properly:\n",
    "scores = cross_val_score(\n",
    "    model_pipe, train.loc[:, train.columns != 'Loan_Approval'], train['Loan_Approval'],\n",
    "    cv=5, scoring='accuracy'\n",
    ")\n",
    "print(\"KNN-impute CV accuracy:\", scores.mean())\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-05-30T10:33:35.323423Z"
    }
   },
   "id": "b96231edbc02c1f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "num_cols = ['Age', 'Years_of_Education', 'Weekly_Work_Hours']          \n",
    "cat_cols = ['Employment_Type', 'Household_Role', 'Preferred_Communication_Method']          \n",
    "knn_col = 'Job_Type'  \n",
    "\n",
    "# 1) Encode all categoricals (including the target column) into ordinal codes:\n",
    "#    We'll impute missing codes later, so give them a placeholder value.\n",
    "cat_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "\n",
    "# 2) KNN imputer\n",
    "knn_imputer = KNNImputer(n_neighbors=3, weights='uniform')\n",
    "\n",
    "# 3) Build a ColumnTransformer that\n",
    "#    - one-hot encodes all categoricals\n",
    "#    - scales numerics\n",
    "#    - then runs the KNN imputer on *all* columns\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('encode_cat', cat_encoder, cat_cols + [knn_col]),\n",
    "    ('scale_num', StandardScaler(), num_cols)\n",
    "])\n",
    "\n",
    "model_pipe = Pipeline([\n",
    "    ('preproc', preprocessor),\n",
    "    ('impute_knn', knn_imputer),\n",
    "    ('clf', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "# 4) Validate properly:\n",
    "scores = cross_val_score(\n",
    "    model_pipe, train.loc[:, train.columns != 'Loan_Approval'], train['Loan_Approval'],\n",
    "    cv=5, scoring='accuracy'\n",
    ")\n",
    "print(\"KNN-impute CV accuracy:\", scores.mean())\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-05-30T10:33:35.324457400Z"
    }
   },
   "id": "e91793537759417c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train.columns"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-05-30T10:33:35.325463500Z"
    }
   },
   "id": "d781284c5c2df25c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train.loc[:, train.columns != 'Loan_Approval'], train['Loan_Approval']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-05-30T10:33:35.327975700Z"
    }
   },
   "id": "49bb1f77795d1b34"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We flagged missing Investment_Gain and saw identical “Missing” vs. “Present” distributions, so it’s MCAR. We will drop those rows to be able to apply Yeo–Johnson to the rest."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "edac95e97289ad40"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "\n",
    "a = train.loc[train['MissingFlag_Job_Type']=='Missing', 'Weekly_Work_Hours'].dropna()\n",
    "b = train.loc[train['MissingFlag_Job_Type']=='Present','Weekly_Work_Hours'].dropna()\n",
    "t_stat, p_val = ttest_ind(a, b, equal_var=False)\n",
    "print(f'p-value = {p_val}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-05-30T10:33:35.328976900Z"
    }
   },
   "id": "d43b1ed5929daff4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#notes:\n",
    "C is handeled with mean bacause its distributed normally."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "79660c63ed10f013"
  },
  {
   "cell_type": "markdown",
   "source": [
    "for values with more than 5 percent of missing data we checked if the data is missed randomly or not.\n",
    "we can see with the t-test that the missing job_type are not random. so we will use a more sofisticated way to deal with the missing values"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f136a7aed9a9fb2c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Training – Basic Models\n",
    "_Start simple. Train Logistic Regression or KNN._"
   ],
   "id": "85618933fc93ce16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# This transforms (missing data and stuff like that) AND does the classifier as well.\n",
    "\n",
    "\n",
    "model = Pipeline([\n",
    "    ('preprocessing', preprocessor),\n",
    "    ('classifier', LogisticRegression())\n",
    "])\n",
    "\n",
    "model.fit(train[[categorical_col]], y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-05-30T10:33:35.329980500Z"
    }
   },
   "id": "1b9e4e1a7e53ba54"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Training – Advanced Models\n",
    "Choose 3 models from:\n",
    "- Decision Tree\n",
    "- Random Forest or AdaBoost\n",
    "- Support Vector Machine\n",
    "- Multi-Layer Perceptron (Neural Net)\n",
    "\n",
    "_Include basic hyperparameter tuning or explain defaults._"
   ],
   "id": "ecc8b1eefecc948c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation\n",
    "- Build confusion matrix for at least one model\n",
    "- Generate ROC curves (using K-Fold CV)\n",
    "- Report AUC\n",
    "\n",
    "**✏️ Answer in markdown:**\n",
    "- Does any model overfit?\n",
    "- Which model generalizes best?\n",
    "- Final choice justification"
   ],
   "id": "65b470cc0d11e7b7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Final Prediction on Test Set\n",
    "_Predict probabilities (not binary labels!) and create submission file._"
   ],
   "id": "1f21f590332511f3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-05-30T10:33:35.332340800Z"
    }
   },
   "outputs": [],
   "source": [
    "# Example:\n",
    "# final_model.predict_proba(test)[:, 1]  # ← probability of 'approved'\n",
    "# submission = pd.DataFrame({\n",
    "#     'customer_id': test['customer_id'],\n",
    "#     'loan_approval': predictions\n",
    "# })\n",
    "# submission.to_csv('Submission_group_32.csv', index=False)"
   ],
   "id": "625f5bbc3680f72a"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Appendix / Extra Notes\n",
    "_Use this area for extra analysis, failed attempts, or ideas you explored._"
   ],
   "id": "ae46b3996890ae43"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
