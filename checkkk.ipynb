{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project – Introduction to Machine Learning (Group 32)\n",
    "## Loan Approval Prediction – Binary Classification\n",
    "**Tamar & Tala – Spring 2025**\n",
    "\n",
    "This notebook walks through our end-to-end machine learning project:\n",
    "- Data exploration \n",
    "- Feature processing \n",
    "- Model training \n",
    "- Evaluation \n",
    "- Final prediction submission "
   ],
   "id": "382a8f2ce576c407"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Libraries and Data\n",
    "_We’ll start by loading the data and setting up useful libraries._"
   ],
   "id": "809af347f383058e"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T12:49:31.411106700Z",
     "start_time": "2025-06-01T12:49:31.252097800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   customer_id   Age Employment_Type       A Education_Level  \\\n0       115892  90.0             NaN   77053         HS-grad   \n1       115893  82.0         Private  132870         HS-grad   \n2       115895  54.0         Private  140359         7th-8th   \n3       115896  41.0         Private  264663    Some-college   \n4       115897  34.0         Private  216864         HS-grad   \n\n   Years_of_Education Marital_Status           Job_Type Household_Role  \\\n0                 9.0        Widowed                NaN  Not-in-family   \n1                 9.0        Widowed    Exec-managerial  Not-in-family   \n2                 4.0       Divorced  Machine-op-inspct      Unmarried   \n3                10.0      Separated     Prof-specialty      Own-child   \n4                 9.0       Divorced      Other-service      Unmarried   \n\n  Ethnicity  ... Investment_Gain  Investment_Loss  Weekly_Work_Hours  \\\n0     White  ...             0.0           4356.0                 40   \n1     White  ...             0.0           4356.0                 18   \n2     White  ...             0.0           3900.0                 40   \n3     White  ...             0.0           3900.0                 40   \n4     White  ...             NaN           3770.0                 45   \n\n   Country_of_Residence           C  Preferred_Communication_Method         D  \\\n0         United-States  104.906221                      Phone_Call  2.865629   \n1         United-States   96.358501                            mail  5.528583   \n2         United-States  115.529631                            mail  3.816915   \n3         United-States   85.732506                             NaN  5.416363   \n4         United-States  115.218443                            Mail  6.453932   \n\n    B           E  Loan_Approval  \n0 NaN  170.887465              0  \n1 NaN  156.630201              0  \n2 NaN  165.635557              0  \n3 NaN  123.379007              0  \n4 NaN  155.262131              0  \n\n[5 rows x 21 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>customer_id</th>\n      <th>Age</th>\n      <th>Employment_Type</th>\n      <th>A</th>\n      <th>Education_Level</th>\n      <th>Years_of_Education</th>\n      <th>Marital_Status</th>\n      <th>Job_Type</th>\n      <th>Household_Role</th>\n      <th>Ethnicity</th>\n      <th>...</th>\n      <th>Investment_Gain</th>\n      <th>Investment_Loss</th>\n      <th>Weekly_Work_Hours</th>\n      <th>Country_of_Residence</th>\n      <th>C</th>\n      <th>Preferred_Communication_Method</th>\n      <th>D</th>\n      <th>B</th>\n      <th>E</th>\n      <th>Loan_Approval</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>115892</td>\n      <td>90.0</td>\n      <td>NaN</td>\n      <td>77053</td>\n      <td>HS-grad</td>\n      <td>9.0</td>\n      <td>Widowed</td>\n      <td>NaN</td>\n      <td>Not-in-family</td>\n      <td>White</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>4356.0</td>\n      <td>40</td>\n      <td>United-States</td>\n      <td>104.906221</td>\n      <td>Phone_Call</td>\n      <td>2.865629</td>\n      <td>NaN</td>\n      <td>170.887465</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>115893</td>\n      <td>82.0</td>\n      <td>Private</td>\n      <td>132870</td>\n      <td>HS-grad</td>\n      <td>9.0</td>\n      <td>Widowed</td>\n      <td>Exec-managerial</td>\n      <td>Not-in-family</td>\n      <td>White</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>4356.0</td>\n      <td>18</td>\n      <td>United-States</td>\n      <td>96.358501</td>\n      <td>mail</td>\n      <td>5.528583</td>\n      <td>NaN</td>\n      <td>156.630201</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>115895</td>\n      <td>54.0</td>\n      <td>Private</td>\n      <td>140359</td>\n      <td>7th-8th</td>\n      <td>4.0</td>\n      <td>Divorced</td>\n      <td>Machine-op-inspct</td>\n      <td>Unmarried</td>\n      <td>White</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>3900.0</td>\n      <td>40</td>\n      <td>United-States</td>\n      <td>115.529631</td>\n      <td>mail</td>\n      <td>3.816915</td>\n      <td>NaN</td>\n      <td>165.635557</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>115896</td>\n      <td>41.0</td>\n      <td>Private</td>\n      <td>264663</td>\n      <td>Some-college</td>\n      <td>10.0</td>\n      <td>Separated</td>\n      <td>Prof-specialty</td>\n      <td>Own-child</td>\n      <td>White</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>3900.0</td>\n      <td>40</td>\n      <td>United-States</td>\n      <td>85.732506</td>\n      <td>NaN</td>\n      <td>5.416363</td>\n      <td>NaN</td>\n      <td>123.379007</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>115897</td>\n      <td>34.0</td>\n      <td>Private</td>\n      <td>216864</td>\n      <td>HS-grad</td>\n      <td>9.0</td>\n      <td>Divorced</td>\n      <td>Other-service</td>\n      <td>Unmarried</td>\n      <td>White</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>3770.0</td>\n      <td>45</td>\n      <td>United-States</td>\n      <td>115.218443</td>\n      <td>Mail</td>\n      <td>6.453932</td>\n      <td>NaN</td>\n      <td>155.262131</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 21 columns</p>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve, classification_report\n",
    "\n",
    "import missingno as msno\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "\n",
    "\n",
    "# Load the data\n",
    "train = pd.read_csv('train.csv', na_values=[\"?\"])\n",
    "test = pd.read_csv('test.csv', na_values=[\"?\"])\n",
    "train.head()"
   ],
   "id": "306e676c18666e27"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis (EDA)\n",
    "_Let's understand the data structure, spot missing values, and explore correlations._\n",
    "\n",
    "\n",
    "**Answer in markdown:**\n",
    "- Are there outliers?\n",
    "- Are there missing values?\n",
    "- Any early ideas on important features?"
   ],
   "id": "1fafd0a08e17e5a7"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "(27676, 21)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_rows,n_columns = train.shape\n",
    "(n_rows,n_columns)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-01T12:49:34.451935100Z",
     "start_time": "2025-06-01T12:49:34.417979800Z"
    }
   },
   "id": "511a2b2c7c2410bf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-06-01T12:48:38.042660400Z"
    }
   },
   "id": "92e56be4784cad8a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train.info()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-01T12:48:38.046148100Z",
     "start_time": "2025-06-01T12:48:38.046148100Z"
    }
   },
   "id": "6bd74af3cb16574"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Column B is completely empty (100% missing), so we will remove it from the dataset.\n",
    "Several columns, such as Employment_Type, Job_Type and Ethnicity have a high percentage of missing values.\n",
    "We'll need to decide whether to fill them or drop them."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9f58dd8d06540027"
  },
  {
   "cell_type": "markdown",
   "source": [
    "_Visualizing the variables to check their distributions:_"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "49bbe0e7870b3c7e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Histograms and count plots of each explanatory variable\n",
    "\n",
    "for col in train.columns:\n",
    "    plt.figure(figsize=(15, 4))\n",
    "    \n",
    "    # For numeric variables\n",
    "    if pd.api.types.is_numeric_dtype(train[col]):\n",
    "        sns.histplot(train[col].dropna(), kde=True)\n",
    "        plt.title(f\"Distribution of {col}\")\n",
    "    \n",
    "    # For categorical variables \n",
    "    else:\n",
    "        sns.countplot(x=col, data=train, order=train[col].value_counts().index)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.title(f\"Countplot of {col}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-01T12:48:38.048573300Z",
     "start_time": "2025-06-01T12:48:38.048573300Z"
    }
   },
   "id": "2f8d6bf1fc5b57f0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Bar plots of approval rates for categorical variables\n",
    "\n",
    "categorical_vars = [\n",
    "    'Employment_Type', 'Education_Level', 'Marital_Status', 'Job_Type',\n",
    "    'Household_Role', 'Ethnicity', 'Gender', 'Country_of_Residence',\n",
    "    'Preferred_Communication_Method'\n",
    "]\n",
    "\n",
    "for col in categorical_vars:\n",
    "    plt.figure(figsize=(15, 4))\n",
    "    sns.barplot(x=col, y='Loan_Approval', data=train, estimator='mean', order=train[col].value_counts().index)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.title(f'Approval Rate by {col}')\n",
    "    plt.ylabel('Approval Rate (Mean of Loan_approval)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-01T12:48:38.073193700Z",
     "start_time": "2025-06-01T12:48:38.051477200Z"
    }
   },
   "id": "42a31bcdb5f5d849"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Box plots of approved vs. rejected for numerical variables\n",
    "train['Loan_Approval_str'] = train['Loan_Approval'].map({0: 'Rejected', 1: 'Approved'})\n",
    "\n",
    "# Remove columns that shouldn't be plotted\n",
    "numeric_cols = [col for col in train.select_dtypes(include='number').columns if col not in ['customer_id', 'Loan_Approval']]\n",
    "\n",
    "# Create one boxplot per feature vs Loan Approval\n",
    "for col in numeric_cols:\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.boxplot(x='Loan_Approval_str', y=col, data=train)\n",
    "    plt.title(f'{col} by Loan Approval')\n",
    "    plt.xlabel('Loan Approval')\n",
    "    plt.ylabel(col)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-06-01T12:48:38.055410500Z"
    }
   },
   "id": "343249a9cee851ed"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Key Insights from Feature Distributions**\n",
    "1. Employment_Type, Country_of_Residence and Ethnicity are highly imbalanced.\n",
    "To reduce sparsity and improve generalization, we will group the less frequent categories into broader, more meaningful groups.\n",
    "2. A is right-skewed, so a log transformation may help normalize its distribution.\n",
    "3. Investment_Gain and Investment_Loss are extremely skewed with large outliers, so a log transformation might help, and merging them into a single Net_Investment feature may provide a clearer signal.\n",
    "4. Preferred_Communication_Method has inconsistent values due to variations in capitalization and wording (e.g., \"Email\" vs \"email\"), so we unified similar values.\n",
    "5. Based on the approval rate bar plot for Household_Role, we observed clear patterns across the categories.\n",
    "We grouped them into three broader categories with similar approval behavior to simplify the feature and improve model stability.\n",
    "6. Column B is completely empty, so we decided to remove it.\n",
    "7.  We simplified the Country_of_Residence feature by grouping it into three categories: USA, Mexico, and Other. The vast majority of people were from the United States, and the approval rate among other countries wasn't very different.\n",
    "8.  We simplified the Employment_Type feature by grouping it into four categories: Private, not private and, and non-working.  The vast majority of people were private, and the noticeable difference in the approval rate was between non-private to non-working.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9e012bc51fce0e55"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Creating a heatmap\n",
    "\n",
    "# Select only numeric columns\n",
    "numeric_df = train.select_dtypes(include='number')\n",
    "\n",
    "# Compute the correlation matrix\n",
    "correlation_matrix = numeric_df.corr()\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", square=True)\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-06-01T12:48:38.058449700Z"
    }
   },
   "id": "4198104465d53ee8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Checking which variables are most correlated with Loan_approval (only linear) - this is the Loan_Approval row in the heatmap\n",
    "numeric_df = train.select_dtypes(include='number')\n",
    "correlations = numeric_df.corr()['Loan_Approval'].sort_values(ascending=False)\n",
    "print(correlations)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-06-01T12:48:38.060444500Z"
    }
   },
   "id": "dd0e681b69d85555"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Key Insights from the heat map**\n",
    "C and E are highly correlated (r = 0.87), indicating redundancy.\n",
    "Neither has a strong linear correlation with the target, but E shows a slightly stronger signal (r = 0.125). Even though E has more missing values (as we will see later on), it’s still more useful, so we’ll keep E and drop C. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "18833f3e4451f9f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We checked for overlap between Employment_Type and Job_Type using a cross-tabulation heatmap.\n",
    "The results show no strong or consistent relationship between the two — most employment types are associated with a wide variety of job types.\n",
    "Therefore, we concluded that the two features capture distinct information and chose to keep both."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dbba21d75f846898"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "crosstab = pd.crosstab(train['Employment_Type'], train['Job_Type'], normalize='index')\n",
    "plt.figure(figsize=(14, 7))\n",
    "sns.heatmap(crosstab, cmap=\"Blues\", annot=True, fmt=\".2f\")\n",
    "plt.title(\"Employment Type vs Job Type\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-06-01T12:48:38.064433800Z"
    }
   },
   "id": "2c2ad593ba0cad71"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Changes based on the key insights:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "114b5f5a38536405"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# 1. Removing columns B and C\n",
    "train = train.drop(['B', 'C'], axis=1)\n",
    "test = test.drop(['B', 'C'], axis=1)\n",
    "n_columns -= 2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-01T12:49:48.391890700Z",
     "start_time": "2025-06-01T12:49:48.363781600Z"
    }
   },
   "id": "e60c0335a6987d0f"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\n# 4) Validate properly:\\nscores = cross_val_score(\\n    job_type_pipe, train.loc[:, train.columns != \\'Loan_Approval\\'], train[\\'Loan_Approval\\'],\\n    cv=5, scoring=\\'accuracy\\'\\n)\\nprint(\"KNN-impute CV accuracy:\", scores.mean())\\n'"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Handle missing Job_Type with knn\n",
    "num_cols = [ 'Years_of_Education']          \n",
    "cat_cols = ['Employment_Type', 'Household_Role']          \n",
    "knn_col = 'Job_Type'  \n",
    "\n",
    "# 1) Encode all categoricals (including the target column) into ordinal codes:\n",
    "#    We'll impute missing codes later, so give them a placeholder value.\n",
    "job_type_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "\n",
    "# 3) Build a ColumnTransformer that\n",
    "#    - one-hot encodes all categoricals\n",
    "#    - scales numerics\n",
    "#    - then runs the KNN imputer on *all* columns\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('encode_cat', job_type_encoder, cat_cols + [knn_col]),\n",
    "    ('scale_num', StandardScaler(), num_cols)\n",
    "])\n",
    "\n",
    "\n",
    "'''\n",
    "# 4) Validate properly:\n",
    "scores = cross_val_score(\n",
    "    job_type_pipe, train.loc[:, train.columns != 'Loan_Approval'], train['Loan_Approval'],\n",
    "    cv=5, scoring='accuracy'\n",
    ")\n",
    "print(\"KNN-impute CV accuracy:\", scores.mean())\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-01T12:50:36.101697300Z",
     "start_time": "2025-06-01T12:50:36.058485500Z"
    }
   },
   "id": "70107e85d41c3754"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\n# 4) Validate properly:\\nscores = cross_val_score(\\n    emp_type_pipe, train.loc[:, train.columns != \\'Loan_Approval\\'], train[\\'Loan_Approval\\'],\\n    cv=5, scoring=\\'accuracy\\'\\n)\\nprint(\"KNN-impute CV accuracy:\", scores.mean())\\n'"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Handle missing Employment_Type with knn\n",
    "num_cols = ['Age', 'Years_of_Education', 'Weekly_Work_Hours']          \n",
    "cat_cols = ['Job_Type', 'Marital_Status']          \n",
    "knn_col = 'Employment_Type'  \n",
    "\n",
    "# 1) Encode all categoricals (including the target column) into ordinal codes:\n",
    "#    We'll impute missing codes later, so give them a placeholder value.\n",
    "emp_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "\n",
    "# 3) Build a ColumnTransformer that\n",
    "#    - one-hot encodes all categoricals\n",
    "#    - scales numerics\n",
    "#    - then runs the KNN imputer on *all* columns\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('encode_cat', emp_encoder, cat_cols + [knn_col]),\n",
    "    ('scale_num', StandardScaler(), num_cols)\n",
    "])\n",
    "\n",
    "\n",
    "'''\n",
    "# 4) Validate properly:\n",
    "scores = cross_val_score(\n",
    "    emp_type_pipe, train.loc[:, train.columns != 'Loan_Approval'], train['Loan_Approval'],\n",
    "    cv=5, scoring='accuracy'\n",
    ")\n",
    "print(\"KNN-impute CV accuracy:\", scores.mean())\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-01T12:50:37.642710700Z",
     "start_time": "2025-06-01T12:50:37.583313300Z"
    }
   },
   "id": "34af00b97f9bb2df"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# ------------ Grouping functions ------------\n",
    "\n",
    "# Grouping Preferred_Communication_Method\n",
    "def group_pref_com(pref):\n",
    "    if pref == \"Mail\" or pref == \"mail\":\n",
    "        return \"Mail\"\n",
    "    if pref == \"Email\" or pref == \"email\":\n",
    "        return \"EMail\"\n",
    "    if pref == \"Phone\" or pref == \"phone\" or pref == \"phone call\":\n",
    "        return \"Phone\"\n",
    "    else:\n",
    "        return \"Other\"\n",
    "\n",
    "# Grouping Country_of_Residence\n",
    "def group_country(country):\n",
    "    if country == \"United-States\":\n",
    "        return \"USA\"\n",
    "    elif country == \"Mexico\":\n",
    "        return \"Mexico\"\n",
    "    else:\n",
    "        return \"Other\"\n",
    "    \n",
    "# Grouping Education_Level\n",
    "def group_education(edu):\n",
    "    pre_school = ['Preschool']\n",
    "    school = ['1st-4th', '5th-6th', '9th', '10th', '11th', '12th', '7th-8th']\n",
    "    post_hs = ['HS-grad', 'Some-college']\n",
    "    assoc = ['Assoc-voc', 'Assoc-acdm']\n",
    "    higher = ['Masters', 'Bachelors']\n",
    "    postgraduate = ['Doctorate', 'Prof-school']\n",
    "    \n",
    "    if edu in pre_school:\n",
    "        return 'pre_school'\n",
    "    elif edu in school:\n",
    "        return 'school'\n",
    "    elif edu in post_hs:\n",
    "        return 'post_hs'\n",
    "    elif edu in assoc:\n",
    "        return 'assoc'\n",
    "    elif edu in higher:\n",
    "        return 'higher'\n",
    "    elif edu in postgraduate:\n",
    "        return 'postgraduate'\n",
    "    else:\n",
    "        return 'Other'\n",
    "    \n",
    "# Grouping Household_role\n",
    "def group_household_role(role):\n",
    "    if role in ['Husband', 'Wife']:\n",
    "        return 'Spouse'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "# Grouping Employment_Type\n",
    "def group_employment(emp):\n",
    "    if emp == 'Private':\n",
    "        return 'Private'\n",
    "    elif emp in ['Self-emp-not-inc', 'Self-emp-inc', 'Local-gov', 'State-gov', 'Federal-gov']:\n",
    "        return 'Not-Private'\n",
    "    elif emp in ['Without-pay', 'Never-worked']:\n",
    "        return 'Non-working'\n",
    "    else:\n",
    "        return 'Other'\n",
    "    \n",
    "# Grouping Ethnicity:\n",
    "def group_ethnicity(eth):\n",
    "    if eth == 'White':\n",
    "        return 'White'\n",
    "    elif eth == 'Black':\n",
    "        return 'Black'\n",
    "    else:\n",
    "        return 'Other'\n",
    "    \n",
    "# 1. Mapping dictionary for Years_of_Education\n",
    "edu_to_years = {\n",
    "    'Preschool'    : 0,\n",
    "    '1st-4th'      : (1 + 2 + 3 + 4) / 4,\n",
    "    '5th-6th'      : 5.5,\n",
    "    '7th-8th'      : 7.5,\n",
    "    '9th'          : 9,\n",
    "    '10th'         : 10,\n",
    "    '11th'         : 11,\n",
    "    '12th'         : 12,\n",
    "    'HS-grad'      : 12,\n",
    "    'Some-college' : (12 + 13 + 14 + 15) / 4,\n",
    "    'Assoc-voc'    : 14,\n",
    "    'Assoc-acdm'   : 14,\n",
    "    'Bachelors'    : 16,\n",
    "    'Masters'      : 18,\n",
    "    'Prof-school'  : 19,\n",
    "    'Doctorate'    : 20\n",
    "}\n",
    "\n",
    "\n",
    "def group_edu_years(X):\n",
    "    #X is a 2D array: [Years_of_Education, Education_Level\n",
    "    \n",
    "    # Convert columns to panda series to be able to map\n",
    "    years_of_education = pd.Series(X[:, 0])\n",
    "    education_level = pd.Series(X[:, 1])\n",
    "    \n",
    "    # Map Education_Level → numeric years\n",
    "    mapped_years = education_level.map(edu_to_years)\n",
    "    \n",
    "    # Fill NaN in the original 'years' with the mapped values\n",
    "    filled = years_of_education.fillna(mapped_years)\n",
    "    \n",
    "    # Return as a single-column DataFrame\n",
    "    return filled.to_frame(name='Years_of_Education')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-01T12:50:38.929287900Z",
     "start_time": "2025-06-01T12:50:38.888453400Z"
    }
   },
   "id": "ea59906540420095"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "# ------------ function transformers ------------\n",
    "\n",
    "pref_com_group = FunctionTransformer(\n",
    "    func=lambda X: pd.Series(X.ravel()).map(group_pref_com).to_frame(),\n",
    "    validate=False\n",
    ")\n",
    "\n",
    "country_group = FunctionTransformer(\n",
    "    func=lambda X: pd.Series(X.ravel()).map(group_country).to_frame(),\n",
    "    validate=False\n",
    ")\n",
    "\n",
    "edu_level_group = FunctionTransformer(\n",
    "    func=lambda X: pd.Series(X.ravel()).map(group_education).to_frame(),\n",
    "    validate=False\n",
    ")\n",
    "\n",
    "house_hold_group = FunctionTransformer(\n",
    "    func=lambda X: pd.Series(X.ravel()).map(group_household_role).to_frame(),\n",
    "    validate=False\n",
    ")\n",
    "\n",
    "emp_type_group = FunctionTransformer(\n",
    "    func=lambda X: pd.Series(X.ravel()).map(group_employment).to_frame(),\n",
    "    validate=False\n",
    ")\n",
    "\n",
    "ethnicity_group = FunctionTransformer(\n",
    "    func=lambda X: pd.Series(X.ravel()).map(group_ethnicity).to_frame(),\n",
    "    validate=False\n",
    ")\n",
    "\n",
    "edu_years_group = FunctionTransformer(\n",
    "    func=group_edu_years,\n",
    "    validate=False\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-01T12:50:45.488628Z",
     "start_time": "2025-06-01T12:50:45.444567200Z"
    }
   },
   "id": "323cef953dcfe6f8"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "# ------------ pipelines ------------\n",
    "\n",
    "knn_imputer = KNNImputer(n_neighbors=3, weights='uniform')\n",
    "\n",
    "# Preferred_Communication_Method pipeline\n",
    "pref_pipeline = Pipeline([\n",
    "    ('group', pref_com_group),  \n",
    "    ('impute', SimpleImputer(strategy='constant', fill_value='Missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Country_of_Residence pipeline\n",
    "country_pipeline = Pipeline([\n",
    "    ('group', country_group),\n",
    "    ('impute', SimpleImputer(strategy='most_frequent')),  # or 'constant' if you prefer\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Education_Level pipeline\n",
    "edu_pipeline = Pipeline([\n",
    "    ('group', edu_level_group),\n",
    "    ('impute', SimpleImputer(strategy='constant', fill_value='Missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Employment_Type pipeline\n",
    "employment_pipeline = Pipeline([\n",
    "    ('group', emp_type_group),\n",
    "    ('impute', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Ethnicity pipeline\n",
    "ethnicity_pipeline = Pipeline([\n",
    "    ('group', ethnicity_group),\n",
    "    ('impute', SimpleImputer(strategy='constant', fill_value='Missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Household pipeline\n",
    "Household_pipeline = Pipeline([\n",
    "    ('group', house_hold_group),\n",
    "    ('impute', SimpleImputer(strategy='constant', fill_value='Missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Years of education pipeline\n",
    "edu_years_pipeline = Pipeline([\n",
    "    ('fill_years', edu_years_group),\n",
    "    ('impute_years', SimpleImputer(strategy='mean'))\n",
    "])\n",
    "\n",
    "# job type of education pipeline\n",
    "job_type_pipe = Pipeline([\n",
    "    ('preproc', preprocessor),\n",
    "    ('impute_knn', knn_imputer),\n",
    "    ('clf', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "# Employment type pipeline\n",
    "emp_type_pipe = Pipeline([\n",
    "    ('preproc', preprocessor),\n",
    "    ('impute_knn', knn_imputer),\n",
    "    ('clf', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "# columns without special adjustments:\n",
    "categorical_col = ['Gender', 'Marital_Status']\n",
    "numerical_col = ['E']\n",
    "skw_numerical_col = ['Age', 'Investment_Gain', 'Investment_Loss']\n",
    "\n",
    "# Pipeline for categorical features\n",
    "cat_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='Missing')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Pipeline for numerical features (mean imputation)\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean'))\n",
    "])\n",
    "\n",
    "# Pipeline for numerical features (median imputation) - for skewed columns\n",
    "skw_numerical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median'))\n",
    "])\n",
    "\n",
    "\n",
    "# Combine pipelines into a ColumnTransformer\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('pref',       pref_pipeline,         ['Preferred_Communication_Method']),\n",
    "    ('country',    country_pipeline,      ['Country_of_Residence']),\n",
    "    ('edu_level',  edu_pipeline,          ['Education_Level']),\n",
    "    ('edu_years',  edu_years_pipeline,    ['Years_of_Education', 'Education_Level']),\n",
    "    ('employment', employment_pipeline,    ['Employment_Type']),\n",
    "    ('ethnicity',  ethnicity_pipeline,    ['Ethnicity']),\n",
    "    ('household',  Household_pipeline,    ['Household_Role']),\n",
    "    ('cat_basic',  cat_pipeline,          ['Gender', 'Marital_Status']),                        # Other categorical columns\n",
    "    ('num_basic',  num_pipeline,          ['E']),                                               # Simple numeric column\n",
    "    ('skw_num',    skw_numerical_pipeline,['Age', 'Investment_Gain', 'Investment_Loss'])        # Skewed numeric columns\n",
    "])\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-01T13:09:31.121547500Z",
     "start_time": "2025-06-01T13:09:31.060735Z"
    }
   },
   "id": "ba45e67358850d1a"
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total missing values after preprocessing: 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = train.drop(columns=['Loan_Approval'])\n",
    "X_transformed = preprocessor.fit_transform(X)\n",
    "\n",
    "\n",
    "# 3. Check for missing values\n",
    "print(\"Total missing values after preprocessing:\", np.isnan(X_transformed).sum())\n",
    "\n",
    "# Optional: break if something is wrong\n",
    "assert not np.isnan(X_transformed).any(), \"❌ Still missing values after preprocessing!\"\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-01T13:32:36.426867700Z",
     "start_time": "2025-06-01T13:32:36.346442600Z"
    }
   },
   "id": "a56ba939da7e6672"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[1.000e+00, 0.000e+00, 0.000e+00, ..., 9.000e+01, 0.000e+00,\n        4.356e+03],\n       [1.000e+00, 0.000e+00, 0.000e+00, ..., 8.200e+01, 0.000e+00,\n        4.356e+03],\n       [1.000e+00, 0.000e+00, 1.000e+00, ..., 5.400e+01, 0.000e+00,\n        3.900e+03],\n       ...,\n       [1.000e+00, 0.000e+00, 0.000e+00, ..., 2.700e+01, 0.000e+00,\n        0.000e+00],\n       [1.000e+00, 0.000e+00, 0.000e+00, ..., 5.800e+01, 0.000e+00,\n        0.000e+00],\n       [0.000e+00, 1.000e+00, 0.000e+00, ..., 2.200e+01, 0.000e+00,\n        0.000e+00]])"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_transformed"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-01T13:11:10.769491200Z",
     "start_time": "2025-06-01T13:11:10.737790300Z"
    }
   },
   "id": "b61e41b475cf182b"
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "data": {
      "text/plain": "       customer_id   Age Employment_Type       A Education_Level  \\\n0           115892  90.0             NaN   77053         HS-grad   \n1           115893  82.0         Private  132870         HS-grad   \n2           115895  54.0         Private  140359         7th-8th   \n3           115896  41.0         Private  264663    Some-college   \n4           115897  34.0         Private  216864         HS-grad   \n...            ...   ...             ...     ...             ...   \n27671       148447  53.0         Private  321865         Masters   \n27672       148448  22.0         Private  310152    Some-college   \n27673       148449  27.0         Private  257302      Assoc-acdm   \n27674       148451  58.0         Private  151910         HS-grad   \n27675       148452  22.0         Private  201490         HS-grad   \n\n       Years_of_Education      Marital_Status           Job_Type  \\\n0                     9.0             Widowed                NaN   \n1                     9.0             Widowed    Exec-managerial   \n2                     4.0            Divorced  Machine-op-inspct   \n3                    10.0           Separated     Prof-specialty   \n4                     9.0            Divorced      Other-service   \n...                   ...                 ...                ...   \n27671                14.0  Married-civ-spouse    Exec-managerial   \n27672                10.0       Never-married    Protective-serv   \n27673                12.0  Married-civ-spouse       Tech-support   \n27674                 9.0             Widowed       Adm-clerical   \n27675                 9.0       Never-married       Adm-clerical   \n\n      Household_Role Ethnicity  Gender  Investment_Gain  Investment_Loss  \\\n0      Not-in-family     White  Female              0.0           4356.0   \n1      Not-in-family     White  Female              0.0           4356.0   \n2          Unmarried     White  Female              0.0           3900.0   \n3          Own-child     White  Female              0.0           3900.0   \n4          Unmarried     White  Female              NaN           3770.0   \n...              ...       ...     ...              ...              ...   \n27671        Husband     White    Male              NaN              0.0   \n27672  Not-in-family     White    Male              0.0              0.0   \n27673           Wife     White  Female              NaN              0.0   \n27674      Unmarried       NaN  Female              0.0              0.0   \n27675      Own-child       NaN    Male              0.0              0.0   \n\n       Weekly_Work_Hours Country_of_Residence Preferred_Communication_Method  \\\n0                     40        United-States                     Phone_Call   \n1                     18        United-States                           mail   \n2                     40        United-States                           mail   \n3                     40        United-States                            NaN   \n4                     45        United-States                           Mail   \n...                  ...                  ...                            ...   \n27671                 40        United-States                           mail   \n27672                 40        United-States                           Mail   \n27673                 38        United-States                          Email   \n27674                 40        United-States                          Email   \n27675                 20        United-States                          email   \n\n              D           E  Loan_Approval  \n0      2.865629  170.887465              0  \n1      5.528583  156.630201              0  \n2      3.816915  165.635557              0  \n3      5.416363  123.379007              0  \n4      6.453932  155.262131              0  \n...         ...         ...            ...  \n27671  6.719535  151.618445              1  \n27672  3.334034  134.985353              0  \n27673  6.204021  148.281633              0  \n27674  2.986238         NaN              0  \n27675  6.204075  145.693908              0  \n\n[27676 rows x 19 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>customer_id</th>\n      <th>Age</th>\n      <th>Employment_Type</th>\n      <th>A</th>\n      <th>Education_Level</th>\n      <th>Years_of_Education</th>\n      <th>Marital_Status</th>\n      <th>Job_Type</th>\n      <th>Household_Role</th>\n      <th>Ethnicity</th>\n      <th>Gender</th>\n      <th>Investment_Gain</th>\n      <th>Investment_Loss</th>\n      <th>Weekly_Work_Hours</th>\n      <th>Country_of_Residence</th>\n      <th>Preferred_Communication_Method</th>\n      <th>D</th>\n      <th>E</th>\n      <th>Loan_Approval</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>115892</td>\n      <td>90.0</td>\n      <td>NaN</td>\n      <td>77053</td>\n      <td>HS-grad</td>\n      <td>9.0</td>\n      <td>Widowed</td>\n      <td>NaN</td>\n      <td>Not-in-family</td>\n      <td>White</td>\n      <td>Female</td>\n      <td>0.0</td>\n      <td>4356.0</td>\n      <td>40</td>\n      <td>United-States</td>\n      <td>Phone_Call</td>\n      <td>2.865629</td>\n      <td>170.887465</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>115893</td>\n      <td>82.0</td>\n      <td>Private</td>\n      <td>132870</td>\n      <td>HS-grad</td>\n      <td>9.0</td>\n      <td>Widowed</td>\n      <td>Exec-managerial</td>\n      <td>Not-in-family</td>\n      <td>White</td>\n      <td>Female</td>\n      <td>0.0</td>\n      <td>4356.0</td>\n      <td>18</td>\n      <td>United-States</td>\n      <td>mail</td>\n      <td>5.528583</td>\n      <td>156.630201</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>115895</td>\n      <td>54.0</td>\n      <td>Private</td>\n      <td>140359</td>\n      <td>7th-8th</td>\n      <td>4.0</td>\n      <td>Divorced</td>\n      <td>Machine-op-inspct</td>\n      <td>Unmarried</td>\n      <td>White</td>\n      <td>Female</td>\n      <td>0.0</td>\n      <td>3900.0</td>\n      <td>40</td>\n      <td>United-States</td>\n      <td>mail</td>\n      <td>3.816915</td>\n      <td>165.635557</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>115896</td>\n      <td>41.0</td>\n      <td>Private</td>\n      <td>264663</td>\n      <td>Some-college</td>\n      <td>10.0</td>\n      <td>Separated</td>\n      <td>Prof-specialty</td>\n      <td>Own-child</td>\n      <td>White</td>\n      <td>Female</td>\n      <td>0.0</td>\n      <td>3900.0</td>\n      <td>40</td>\n      <td>United-States</td>\n      <td>NaN</td>\n      <td>5.416363</td>\n      <td>123.379007</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>115897</td>\n      <td>34.0</td>\n      <td>Private</td>\n      <td>216864</td>\n      <td>HS-grad</td>\n      <td>9.0</td>\n      <td>Divorced</td>\n      <td>Other-service</td>\n      <td>Unmarried</td>\n      <td>White</td>\n      <td>Female</td>\n      <td>NaN</td>\n      <td>3770.0</td>\n      <td>45</td>\n      <td>United-States</td>\n      <td>Mail</td>\n      <td>6.453932</td>\n      <td>155.262131</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>27671</th>\n      <td>148447</td>\n      <td>53.0</td>\n      <td>Private</td>\n      <td>321865</td>\n      <td>Masters</td>\n      <td>14.0</td>\n      <td>Married-civ-spouse</td>\n      <td>Exec-managerial</td>\n      <td>Husband</td>\n      <td>White</td>\n      <td>Male</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>40</td>\n      <td>United-States</td>\n      <td>mail</td>\n      <td>6.719535</td>\n      <td>151.618445</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>27672</th>\n      <td>148448</td>\n      <td>22.0</td>\n      <td>Private</td>\n      <td>310152</td>\n      <td>Some-college</td>\n      <td>10.0</td>\n      <td>Never-married</td>\n      <td>Protective-serv</td>\n      <td>Not-in-family</td>\n      <td>White</td>\n      <td>Male</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>40</td>\n      <td>United-States</td>\n      <td>Mail</td>\n      <td>3.334034</td>\n      <td>134.985353</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>27673</th>\n      <td>148449</td>\n      <td>27.0</td>\n      <td>Private</td>\n      <td>257302</td>\n      <td>Assoc-acdm</td>\n      <td>12.0</td>\n      <td>Married-civ-spouse</td>\n      <td>Tech-support</td>\n      <td>Wife</td>\n      <td>White</td>\n      <td>Female</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>38</td>\n      <td>United-States</td>\n      <td>Email</td>\n      <td>6.204021</td>\n      <td>148.281633</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>27674</th>\n      <td>148451</td>\n      <td>58.0</td>\n      <td>Private</td>\n      <td>151910</td>\n      <td>HS-grad</td>\n      <td>9.0</td>\n      <td>Widowed</td>\n      <td>Adm-clerical</td>\n      <td>Unmarried</td>\n      <td>NaN</td>\n      <td>Female</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>40</td>\n      <td>United-States</td>\n      <td>Email</td>\n      <td>2.986238</td>\n      <td>NaN</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>27675</th>\n      <td>148452</td>\n      <td>22.0</td>\n      <td>Private</td>\n      <td>201490</td>\n      <td>HS-grad</td>\n      <td>9.0</td>\n      <td>Never-married</td>\n      <td>Adm-clerical</td>\n      <td>Own-child</td>\n      <td>NaN</td>\n      <td>Male</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>20</td>\n      <td>United-States</td>\n      <td>email</td>\n      <td>6.204075</td>\n      <td>145.693908</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>27676 rows × 19 columns</p>\n</div>"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-01T13:32:57.551525100Z",
     "start_time": "2025-06-01T13:32:57.477261700Z"
    }
   },
   "id": "e0acc6746079d24c"
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     0.0\n",
      "1     0.0\n",
      "2     0.0\n",
      "3     0.0\n",
      "4     0.0\n",
      "5     0.0\n",
      "6     0.0\n",
      "7     0.0\n",
      "8     0.0\n",
      "9     0.0\n",
      "10    0.0\n",
      "11    0.0\n",
      "12    0.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Understanding missing values - Check how many missing values there are in each column and if it has a pattern\n",
    "missing = X_transformed_df.isnull().sum()\n",
    "missing_percent = (missing / len(train)) * 100\n",
    "\n",
    "print(missing_percent.sort_values(ascending=False))\n",
    "\n",
    "# Plot missing value matrix\n",
    "#msno.matrix(train)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-01T13:13:54.161104700Z",
     "start_time": "2025-06-01T13:13:54.114174900Z"
    }
   },
   "id": "2b1e7f7d4ef04a07"
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "data": {
      "text/plain": "   customer_id   Age Employment_Type       A Education_Level  \\\n0       115892  90.0             NaN   77053         HS-grad   \n1       115893  82.0         Private  132870         HS-grad   \n2       115895  54.0         Private  140359         7th-8th   \n3       115896  41.0         Private  264663    Some-college   \n4       115897  34.0         Private  216864         HS-grad   \n\n   Years_of_Education Marital_Status           Job_Type Household_Role  \\\n0                 9.0        Widowed                NaN  Not-in-family   \n1                 9.0        Widowed    Exec-managerial  Not-in-family   \n2                 4.0       Divorced  Machine-op-inspct      Unmarried   \n3                10.0      Separated     Prof-specialty      Own-child   \n4                 9.0       Divorced      Other-service      Unmarried   \n\n  Ethnicity  Gender  Investment_Gain  Investment_Loss  Weekly_Work_Hours  \\\n0     White  Female              0.0           4356.0                 40   \n1     White  Female              0.0           4356.0                 18   \n2     White  Female              0.0           3900.0                 40   \n3     White  Female              0.0           3900.0                 40   \n4     White  Female              NaN           3770.0                 45   \n\n  Country_of_Residence Preferred_Communication_Method         D           E  \\\n0        United-States                     Phone_Call  2.865629  170.887465   \n1        United-States                           mail  5.528583  156.630201   \n2        United-States                           mail  3.816915  165.635557   \n3        United-States                            NaN  5.416363  123.379007   \n4        United-States                           Mail  6.453932  155.262131   \n\n   Loan_Approval  \n0              0  \n1              0  \n2              0  \n3              0  \n4              0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>customer_id</th>\n      <th>Age</th>\n      <th>Employment_Type</th>\n      <th>A</th>\n      <th>Education_Level</th>\n      <th>Years_of_Education</th>\n      <th>Marital_Status</th>\n      <th>Job_Type</th>\n      <th>Household_Role</th>\n      <th>Ethnicity</th>\n      <th>Gender</th>\n      <th>Investment_Gain</th>\n      <th>Investment_Loss</th>\n      <th>Weekly_Work_Hours</th>\n      <th>Country_of_Residence</th>\n      <th>Preferred_Communication_Method</th>\n      <th>D</th>\n      <th>E</th>\n      <th>Loan_Approval</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>115892</td>\n      <td>90.0</td>\n      <td>NaN</td>\n      <td>77053</td>\n      <td>HS-grad</td>\n      <td>9.0</td>\n      <td>Widowed</td>\n      <td>NaN</td>\n      <td>Not-in-family</td>\n      <td>White</td>\n      <td>Female</td>\n      <td>0.0</td>\n      <td>4356.0</td>\n      <td>40</td>\n      <td>United-States</td>\n      <td>Phone_Call</td>\n      <td>2.865629</td>\n      <td>170.887465</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>115893</td>\n      <td>82.0</td>\n      <td>Private</td>\n      <td>132870</td>\n      <td>HS-grad</td>\n      <td>9.0</td>\n      <td>Widowed</td>\n      <td>Exec-managerial</td>\n      <td>Not-in-family</td>\n      <td>White</td>\n      <td>Female</td>\n      <td>0.0</td>\n      <td>4356.0</td>\n      <td>18</td>\n      <td>United-States</td>\n      <td>mail</td>\n      <td>5.528583</td>\n      <td>156.630201</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>115895</td>\n      <td>54.0</td>\n      <td>Private</td>\n      <td>140359</td>\n      <td>7th-8th</td>\n      <td>4.0</td>\n      <td>Divorced</td>\n      <td>Machine-op-inspct</td>\n      <td>Unmarried</td>\n      <td>White</td>\n      <td>Female</td>\n      <td>0.0</td>\n      <td>3900.0</td>\n      <td>40</td>\n      <td>United-States</td>\n      <td>mail</td>\n      <td>3.816915</td>\n      <td>165.635557</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>115896</td>\n      <td>41.0</td>\n      <td>Private</td>\n      <td>264663</td>\n      <td>Some-college</td>\n      <td>10.0</td>\n      <td>Separated</td>\n      <td>Prof-specialty</td>\n      <td>Own-child</td>\n      <td>White</td>\n      <td>Female</td>\n      <td>0.0</td>\n      <td>3900.0</td>\n      <td>40</td>\n      <td>United-States</td>\n      <td>NaN</td>\n      <td>5.416363</td>\n      <td>123.379007</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>115897</td>\n      <td>34.0</td>\n      <td>Private</td>\n      <td>216864</td>\n      <td>HS-grad</td>\n      <td>9.0</td>\n      <td>Divorced</td>\n      <td>Other-service</td>\n      <td>Unmarried</td>\n      <td>White</td>\n      <td>Female</td>\n      <td>NaN</td>\n      <td>3770.0</td>\n      <td>45</td>\n      <td>United-States</td>\n      <td>Mail</td>\n      <td>6.453932</td>\n      <td>155.262131</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-01T13:14:52.457380Z",
     "start_time": "2025-06-01T13:14:52.383841900Z"
    }
   },
   "id": "876956a066e3ddf1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "#Doing a log transformation while still distinguishing losses vs. gains.\n",
    "train['Net_Investment'] = train['Investment_Gain'] - train['Investment_Loss']\n",
    "test['Net_Investment'] = test['Investment_Gain'] - test['Investment_Loss']\n",
    "\n",
    "train['Log_Net_Investment'] = np.sign(train['Net_Investment']) * np.log2(abs(train['Net_Investment']) + 1)\n",
    "test['Log_Net_Investment'] = np.sign(test['Net_Investment']) * np.log2(abs(test['Net_Investment']) + 1)\n",
    "\n",
    "\n",
    "# Side-by-side boxplots \n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "sns.boxplot(x=train['Net_Investment'], ax=axes[0])\n",
    "axes[0].set_title('Original Net_Investment')\n",
    "axes[0].set_xlabel('Net_Investment')\n",
    "\n",
    "sns.boxplot(x=train['Log_Net_Investment'], ax=axes[1])\n",
    "axes[1].set_title('Log-Transformed Net_Investment')\n",
    "axes[1].set_xlabel('Log_Net_Investment')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Side-by-side histograms\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "sns.histplot(train['Net_Investment'].dropna(), kde=True, ax=axes[0])\n",
    "axes[0].set_title('Distribution of Net_Investment')\n",
    "axes[0].set_xlabel('Net_Investment')\n",
    "\n",
    "sns.histplot(train['Log_Net_Investment'].dropna(), kde=True, ax=axes[1])\n",
    "axes[1].set_title('Distribution of Log-Transformed Net_Investment')\n",
    "axes[1].set_xlabel('Log_Net_Investment')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-01T12:48:38.159956400Z",
     "start_time": "2025-06-01T12:48:38.095544100Z"
    }
   },
   "id": "572f1caebdf1dfa5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train['log_gain'] = np.log2(train['Investment_Gain'] + 1)\n",
    "train['log_loss'] = np.log2(train['Investment_Loss'] + 1)\n",
    "train['net_investment_log'] = train['log_gain'] - train['log_loss']\n",
    "\n",
    "plt.hist(train['net_investment_log'])\n",
    "plt.show()\n",
    "\n",
    "sns.histplot(train['net_investment_log'].dropna(), kde=True)\n",
    "plt.title(f\"Distribution of net_investment_log\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-06-01T12:48:38.098090800Z"
    }
   },
   "id": "bc94fa2c0e4a4e18"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We now have a few ways of dealing with the investments: \n",
    "1. keeping each one in the original way.\n",
    "2. keeping each one with a log transformation.\n",
    "3. Log on each one and then difference\n",
    "4. Difference and then log on each one.\n",
    "\n",
    "we will need to do some cross-validation and check which one gives the best results."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c4e7a8147e8d9782"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We applied a log transformation to Net_Investment to reduce skew and compress extreme outliers.\n",
    "The resulting distribution is more symmetric, so we will keep it.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dcc19d2573d9a90b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We applied a log transformation to column A to reduce skew and potentially improve its relationship with the target.\n",
    "However, the correlation with Loan_Approval remained low, so we won't use it."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dcaf1ff96686fb72"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pd.crosstab(train['Loan_Approval'], train['Gender'])\n",
    "pd.crosstab(train['Loan_Approval'], train['Ethnicity'], normalize='index')\n",
    "pd.crosstab(train['Loan_Approval'], train['Education_Level'], normalize='index')\n",
    "pd.crosstab(train['Loan_Approval_str'], train['Gender'], normalize='index')\n",
    "\n",
    "# Doesn't really help because of the unbalanced proportions. Add an explanation.\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-06-01T12:48:38.099523Z"
    }
   },
   "id": "ecc695970f3c9fe"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# AFTER CHANGING \n",
    "\n",
    "# Checking which variables are most correlated with Loan_approval (only linear):\n",
    "numeric_df = train.select_dtypes(include='number')\n",
    "correlations = numeric_df.corr()['Loan_Approval'].sort_values(ascending=False)\n",
    "print(correlations)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-06-01T12:48:38.101531600Z"
    }
   },
   "id": "1d1ac20a311400d4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Data Preprocessing\n",
    "_Handle missing values, encode categorical features, normalize, etc._\n",
    "\n",
    "**✏️ Answer in markdown:**\n",
    "- Did you normalize the data? Why?\n",
    "- How did you handle categorical variables?\n",
    "- Did you apply PCA or feature selection?\n",
    "- Did you drop or create new features?\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aa6ac0592772b405"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "######## TRY AGAIN AFTER ADDING AGE\n",
    "'''\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "\n",
    "df = train  \n",
    "\n",
    "# 1. Define features & split known vs missing\n",
    "features = ['Weekly_Work_Hours', 'Years_of_Education', 'Age']\n",
    "known   = df[df['Investment_Gain'].notnull() ].copy()\n",
    "missing = df[df['Investment_Gain'].isnull()].copy()\n",
    "\n",
    "X_known = known[features]\n",
    "y_known = known['Investment_Gain']\n",
    "\n",
    "# 2. Hold out 20% for validation\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    X_known, y_known,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 3. Train on the 80%\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_tr, y_tr)\n",
    "\n",
    "# 4. Predict on the hold‐out and evaluate\n",
    "y_pred = lr.predict(X_val)\n",
    "print(f\"R² on hold-out: {r2_score(y_val, y_pred):.3f}\")\n",
    "print(f\"MAE on hold-out: {mean_absolute_error(y_val, y_pred):.3f}\")\n",
    "\n",
    "# 5. (Optional) Retrain on ALL known data and impute missings\n",
    "lr_full = LinearRegression()\n",
    "lr_full.fit(X_known, y_known)\n",
    "\n",
    "X_missing = missing[features]\n",
    "df.loc[df['Investment_Gain'].isnull(), 'Investment_Gain'] = lr_full.predict(X_missing)\n",
    "'''\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-06-01T12:48:38.104579300Z"
    }
   },
   "id": "582f303fb88677b8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We tried to handle the missing values with linear regression, but R^2 was 0.02, so we just impute with the median."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1bb220032e4c5488"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The plot shows that column B is completely empty, so we should remove it. The rest of the columns have some missing values, but there doesn’t seem to be any clear pattern."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bfde1cd4ed56b68c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 1. Create a boolean mask for missingness - job type\n",
    "mask_Job_Type = train['Job_Type'].isna()            # True where missing\n",
    "train['MissingFlag_Job_Type'] = mask_Job_Type.map({True: 'Missing', False: 'Present'})\n",
    "\n",
    "# 2. Summary statistics for numeric vars\n",
    "numeric_cols = ['Log_Net_Investment']\n",
    "#numeric_cols = ['Age', 'Years_of_Education', 'Weekly_Work_Hours', 'Net_Investment', 'A', 'D', 'E']\n",
    "print(train.groupby('MissingFlag_Job_Type')[numeric_cols].describe().unstack(1))\n",
    "\n",
    "''' bring back after - just sick of the big graphs.\n",
    "# 4. Frequencies for other categorical vars\n",
    "cat_cols = ['Marital_Status', 'Household_Role', ]\n",
    "for col in cat_cols:\n",
    "    freq = pd.crosstab(train['MissingFlag_Job_Type'], train[col], normalize='index')\n",
    "    print(f'\\nRelative frequency of {col}:\\n', freq)\n",
    "    freq.plot.bar(stacked=True)\n",
    "    plt.title(f'{col} by missingness')\n",
    "    plt.ylabel('Proportion')\n",
    "    plt.show()\n",
    "\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-06-01T12:48:38.107918Z"
    }
   },
   "id": "90178773caacf0f3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 1. Create a boolean mask for missingness with Investment_Gain to see if we can delete it to use YJ\n",
    "mask_Investment_Gain = train['Investment_Gain'].isna()            # True where missing\n",
    "train['MissingFlag_Investment_Gain'] = mask_Investment_Gain.map({True: 'Missing', False: 'Present'})\n",
    "\n",
    "# 2. Summary statistics for numeric vars\n",
    "numeric_cols = ['E']\n",
    "#numeric_cols = ['Age', 'Years_of_Education', 'Weekly_Work_Hours', 'A', 'D', 'E']\n",
    "print(train.groupby('MissingFlag_Investment_Gain')[numeric_cols].describe().unstack(1))\n",
    "\n",
    "\n",
    "''' bring back after - just sick of the big graphs.\n",
    "\n",
    "# 4. Frequencies for other categorical vars\n",
    "cat_cols = ['Marital_Status', 'Household_Role', ]\n",
    "for col in cat_cols:\n",
    "    freq = pd.crosstab(train['MissingFlag_Investment_Gain'], train[col], normalize='index')\n",
    "    print(f'\\nRelative frequency of {col}:\\n', freq)\n",
    "    freq.plot.bar(stacked=True)\n",
    "    plt.title(f'{col} by missingness')\n",
    "    plt.ylabel('Proportion')\n",
    "    plt.show()\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-06-01T12:48:38.109077800Z"
    }
   },
   "id": "4ec05495035e859f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 1. Create a boolean mask for missingness with Ethnicity\n",
    "mask_Ethnicity = train['Ethnicity'].isna()            # True where missing\n",
    "train['MissingFlag_Ethnicity'] = mask_Ethnicity.map({True: 'Missing', False: 'Present'})\n",
    "\n",
    "# 2. Summary statistics for numeric vars\n",
    "numeric_cols = ['D']\n",
    "#numeric_cols = ['Age', 'Years_of_Education', 'Weekly_Work_Hours', 'A', 'D', 'E']\n",
    "print(train.groupby('MissingFlag_Ethnicity')[numeric_cols].describe().unstack(1))\n",
    "\n",
    "'''\n",
    "#bring back after - just sick of the big graphs.\n",
    "\n",
    "# 4. Frequencies for other categorical vars\n",
    "cat_cols = ['Marital_Status', 'Household_Role', 'Employment_Type', 'Job_Type', 'Gender', 'Country_of_Residence', 'Preferred_Communication_Method']\n",
    "for col in cat_cols:\n",
    "    freq = pd.crosstab(train['MissingFlag_Ethnicity'], train[col], normalize='index')\n",
    "    print(f'\\nRelative frequency of {col}:\\n', freq)\n",
    "    freq.plot.bar(stacked=True)\n",
    "    plt.title(f'{col} by missingness')\n",
    "    plt.ylabel('Proportion')\n",
    "    plt.show()\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-06-01T12:48:38.111104800Z"
    }
   },
   "id": "8bffe75d52c78054"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 1. Create a boolean mask for missingness with Ethnicity\n",
    "mask_Ethnicity = train['Preferred_Communication_Method'].isna()            # True where missing\n",
    "train['MissingFlag_Ethnicity'] = mask_Ethnicity.map({True: 'Missing', False: 'Present'})\n",
    "\n",
    "# 2. Summary statistics for numeric vars\n",
    "numeric_cols = ['Weekly_Work_Hours']\n",
    "#numeric_cols = ['Age', 'Years_of_Education', 'Weekly_Work_Hours', 'A', 'D', 'E']\n",
    "print(train.groupby('MissingFlag_Ethnicity')[numeric_cols].describe().unstack(1))\n",
    "\n",
    "'''\n",
    "#bring back after - just sick of the big graphs.\n",
    "\n",
    "# 4. Frequencies for other categorical vars\n",
    "cat_cols = ['Marital_Status', 'Household_Role', 'Employment_Type', 'Job_Type', 'Gender', 'Country_of_Residence', 'Preferred_Communication_Method']\n",
    "for col in cat_cols:\n",
    "    freq = pd.crosstab(train['MissingFlag_Ethnicity'], train[col], normalize='index')\n",
    "    print(f'\\nRelative frequency of {col}:\\n', freq)\n",
    "    freq.plot.bar(stacked=True)\n",
    "    plt.title(f'{col} by missingness')\n",
    "    plt.ylabel('Proportion')\n",
    "    plt.show()\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-06-01T12:48:38.112109400Z"
    }
   },
   "id": "ebcdeacd8c6f8288"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 1. Create a boolean mask for Job tyoe with Ethnicity\n",
    "mask_Ethnicity = train['Job_Type'].isna()            # True where missing\n",
    "train['MissingFlag_Ethnicity'] = mask_Ethnicity.map({True: 'Missing', False: 'Present'})\n",
    "\n",
    "# 2. Summary statistics for numeric vars\n",
    "numeric_cols = ['Weekly_Work_Hours']\n",
    "#numeric_cols = ['Age', 'Years_of_Education', 'Weekly_Work_Hours', 'A', 'D', 'E']\n",
    "print(train.groupby('MissingFlag_Ethnicity')[numeric_cols].describe().unstack(1))\n",
    "\n",
    "'''\n",
    "#bring back after - just sick of the big graphs.\n",
    "\n",
    "# 4. Frequencies for other categorical vars\n",
    "cat_cols = ['Marital_Status', 'Household_Role', 'Employment_Type', 'Job_Type', 'Gender', 'Country_of_Residence', 'Preferred_Communication_Method']\n",
    "for col in cat_cols:\n",
    "    freq = pd.crosstab(train['MissingFlag_Ethnicity'], train[col], normalize='index')\n",
    "    print(f'\\nRelative frequency of {col}:\\n', freq)\n",
    "    freq.plot.bar(stacked=True)\n",
    "    plt.title(f'{col} by missingness')\n",
    "    plt.ylabel('Proportion')\n",
    "    plt.show()\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-06-01T12:48:38.114495100Z"
    }
   },
   "id": "dc682854a7545428"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 1. Create a boolean mask for missingness with Ethnicity\n",
    "mask_Ethnicity = train['Employment_Type'].isna()            # True where missing\n",
    "train['MissingFlag_Ethnicity'] = mask_Ethnicity.map({True: 'Missing', False: 'Present'})\n",
    "\n",
    "# 2. Summary statistics for numeric vars\n",
    "numeric_cols = ['Weekly_Work_Hours']\n",
    "#numeric_cols = ['Age', 'Years_of_Education', 'Weekly_Work_Hours', 'A', 'D', 'E']\n",
    "print(train.groupby('MissingFlag_Ethnicity')[numeric_cols].describe().unstack(1))\n",
    "\n",
    "'''\n",
    "#bring back after - just sick of the big graphs.\n",
    "\n",
    "# 4. Frequencies for other categorical vars\n",
    "cat_cols = ['Marital_Status', 'Household_Role', 'Employment_Type', 'Job_Type', 'Gender', 'Country_of_Residence', 'Preferred_Communication_Method']\n",
    "for col in cat_cols:\n",
    "    freq = pd.crosstab(train['MissingFlag_Ethnicity'], train[col], normalize='index')\n",
    "    print(f'\\nRelative frequency of {col}:\\n', freq)\n",
    "    freq.plot.bar(stacked=True)\n",
    "    plt.title(f'{col} by missingness')\n",
    "    plt.ylabel('Proportion')\n",
    "    plt.show()\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-06-01T12:48:38.115570900Z"
    }
   },
   "id": "2f9d561e22c26d01"
  },
  {
   "cell_type": "markdown",
   "source": [
    "i see that the missing and present have a really different distribution. so i used knn"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "904a6fcd72dab2c4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We flagged missing Investment_Gain and saw identical “Missing” vs. “Present” distributions, so it’s MCAR. We will drop those rows to be able to apply Yeo–Johnson to the rest.- DONT THINK SO"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "edac95e97289ad40"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "\n",
    "a = train.loc[train['MissingFlag_Job_Type']=='Missing', 'Weekly_Work_Hours'].dropna()\n",
    "b = train.loc[train['MissingFlag_Job_Type']=='Present','Weekly_Work_Hours'].dropna()\n",
    "t_stat, p_val = ttest_ind(a, b, equal_var=False)\n",
    "print(f'p-value = {p_val}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-06-01T12:48:38.118046700Z"
    }
   },
   "id": "d43b1ed5929daff4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#notes:\n",
    "C is handeled with mean bacause its distributed normally."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "79660c63ed10f013"
  },
  {
   "cell_type": "markdown",
   "source": [
    "for values with more than 5 percent of missing data we checked if the data is missed randomly or not.\n",
    "we can see with the t-test that the missing job_type are not random. so we will use a more sofisticated way to deal with the missing values"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f136a7aed9a9fb2c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Training – Basic Models\n",
    "_Start simple. Train Logistic Regression or KNN._"
   ],
   "id": "85618933fc93ce16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# This transforms (missing data and stuff like that) AND does the classifier as well.\n",
    "\n",
    "\n",
    "model = Pipeline([\n",
    "    ('preprocessing', preprocessor),\n",
    "    ('classifier', LogisticRegression())\n",
    "])\n",
    "\n",
    "model.fit(train[[categorical_col]], y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-06-01T12:48:38.120068300Z"
    }
   },
   "id": "1b9e4e1a7e53ba54"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Training – Advanced Models\n",
    "Choose 3 models from:\n",
    "- Decision Tree\n",
    "- Random Forest or AdaBoost\n",
    "- Support Vector Machine\n",
    "- Multi-Layer Perceptron (Neural Net)\n",
    "\n",
    "_Include basic hyperparameter tuning or explain defaults._"
   ],
   "id": "ecc8b1eefecc948c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation\n",
    "- Build confusion matrix for at least one model\n",
    "- Generate ROC curves (using K-Fold CV)\n",
    "- Report AUC\n",
    "\n",
    "**✏️ Answer in markdown:**\n",
    "- Does any model overfit?\n",
    "- Which model generalizes best?\n",
    "- Final choice justification"
   ],
   "id": "65b470cc0d11e7b7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Final Prediction on Test Set\n",
    "_Predict probabilities (not binary labels!) and create submission file._"
   ],
   "id": "1f21f590332511f3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-06-01T12:48:38.121072800Z"
    }
   },
   "outputs": [],
   "source": [
    "# Example:\n",
    "# final_model.predict_proba(test)[:, 1]  # ← probability of 'approved'\n",
    "# submission = pd.DataFrame({\n",
    "#     'customer_id': test['customer_id'],\n",
    "#     'loan_approval': predictions\n",
    "# })\n",
    "# submission.to_csv('Submission_group_32.csv', index=False)"
   ],
   "id": "625f5bbc3680f72a"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Appendix / Extra Notes\n",
    "_Use this area for extra analysis, failed attempts, or ideas you explored._"
   ],
   "id": "ae46b3996890ae43"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
