{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project – Introduction to Machine Learning (Group 32)\n",
    "## Loan Approval Prediction – Binary Classification\n",
    "**Tamar & Tala – Spring 2025**\n",
    "\n",
    "This notebook walks through our end-to-end machine learning project:\n",
    "- Data exploration \n",
    "- Feature processing \n",
    "- Model training \n",
    "- Evaluation \n",
    "- Final prediction submission "
   ],
   "id": "382a8f2ce576c407"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Libraries and Data\n",
    "_We’ll start by loading the data and setting up useful libraries._"
   ],
   "id": "809af347f383058e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve, classification_report\n",
    "\n",
    "import missingno as msno\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "\n",
    "\n",
    "# Load the data\n",
    "train = pd.read_csv('train.csv', na_values=[\"?\"])\n",
    "test = pd.read_csv('test.csv', na_values=[\"?\"])\n",
    "train.head()"
   ],
   "id": "306e676c18666e27"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis (EDA)\n",
    "_Let's understand the data structure, spot missing values, and explore correlations._\n",
    "\n",
    "\n",
    "**Answer in markdown:**\n",
    "- Are there outliers?\n",
    "- Are there missing values?\n",
    "- Any early ideas on important features?"
   ],
   "id": "1fafd0a08e17e5a7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n_rows,n_columns = train.shape\n",
    "(n_rows,n_columns)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "511a2b2c7c2410bf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-06-01T12:48:38.042660400Z"
    }
   },
   "id": "92e56be4784cad8a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train.info()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-01T12:48:38.046148100Z",
     "start_time": "2025-06-01T12:48:38.046148100Z"
    }
   },
   "id": "6bd74af3cb16574"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Column B is completely empty (100% missing), so we will remove it from the dataset.\n",
    "Several columns, such as Employment_Type, Job_Type and Ethnicity have a high percentage of missing values.\n",
    "We'll need to decide whether to fill them or drop them."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9f58dd8d06540027"
  },
  {
   "cell_type": "markdown",
   "source": [
    "_Visualizing the variables to check their distributions:_"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "49bbe0e7870b3c7e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Histograms and count plots of each explanatory variable\n",
    "\n",
    "for col in train.columns:\n",
    "    plt.figure(figsize=(15, 4))\n",
    "    \n",
    "    # For numeric variables\n",
    "    if pd.api.types.is_numeric_dtype(train[col]):\n",
    "        sns.histplot(train[col].dropna(), kde=True)\n",
    "        plt.title(f\"Distribution of {col}\")\n",
    "    \n",
    "    # For categorical variables \n",
    "    else:\n",
    "        sns.countplot(x=col, data=train, order=train[col].value_counts().index)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.title(f\"Countplot of {col}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-01T12:48:38.048573300Z",
     "start_time": "2025-06-01T12:48:38.048573300Z"
    }
   },
   "id": "2f8d6bf1fc5b57f0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Bar plots of approval rates for categorical variables\n",
    "\n",
    "categorical_vars = [\n",
    "    'Employment_Type', 'Education_Level', 'Marital_Status', 'Job_Type',\n",
    "    'Household_Role', 'Ethnicity', 'Gender', 'Country_of_Residence',\n",
    "    'Preferred_Communication_Method'\n",
    "]\n",
    "\n",
    "for col in categorical_vars:\n",
    "    plt.figure(figsize=(15, 4))\n",
    "    sns.barplot(x=col, y='Loan_Approval', data=train, estimator='mean', order=train[col].value_counts().index)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.title(f'Approval Rate by {col}')\n",
    "    plt.ylabel('Approval Rate (Mean of Loan_approval)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-01T12:48:38.073193700Z",
     "start_time": "2025-06-01T12:48:38.051477200Z"
    }
   },
   "id": "42a31bcdb5f5d849"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Box plots of approved vs. rejected for numerical variables\n",
    "train['Loan_Approval_str'] = train['Loan_Approval'].map({0: 'Rejected', 1: 'Approved'})\n",
    "\n",
    "# Remove columns that shouldn't be plotted\n",
    "numeric_cols = [col for col in train.select_dtypes(include='number').columns if col not in ['customer_id', 'Loan_Approval']]\n",
    "\n",
    "# Create one boxplot per feature vs Loan Approval\n",
    "for col in numeric_cols:\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.boxplot(x='Loan_Approval_str', y=col, data=train)\n",
    "    plt.title(f'{col} by Loan Approval')\n",
    "    plt.xlabel('Loan Approval')\n",
    "    plt.ylabel(col)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-06-01T12:48:38.055410500Z"
    }
   },
   "id": "343249a9cee851ed"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Key Insights from Feature Distributions**\n",
    "1. Employment_Type, Country_of_Residence and Ethnicity are highly imbalanced.\n",
    "To reduce sparsity and improve generalization, we will group the less frequent categories into broader, more meaningful groups.\n",
    "2. A is right-skewed, so a log transformation may help normalize its distribution.\n",
    "3. Investment_Gain and Investment_Loss are extremely skewed with large outliers, so a log transformation might help, and merging them into a single Net_Investment feature may provide a clearer signal.\n",
    "4. Preferred_Communication_Method has inconsistent values due to variations in capitalization and wording (e.g., \"Email\" vs \"email\"), so we unified similar values.\n",
    "5. Based on the approval rate bar plot for Household_Role, we observed clear patterns across the categories.\n",
    "We grouped them into three broader categories with similar approval behavior to simplify the feature and improve model stability.\n",
    "6. Column B is completely empty, so we decided to remove it.\n",
    "7.  We simplified the Country_of_Residence feature by grouping it into three categories: USA, Mexico, and Other. The vast majority of people were from the United States, and the approval rate among other countries wasn't very different.\n",
    "8.  We simplified the Employment_Type feature by grouping it into four categories: Private, not private and, and non-working.  The vast majority of people were private, and the noticeable difference in the approval rate was between non-private to non-working.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9e012bc51fce0e55"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Creating a heatmap\n",
    "\n",
    "# Select only numeric columns\n",
    "numeric_df = train.select_dtypes(include='number')\n",
    "\n",
    "# Compute the correlation matrix\n",
    "correlation_matrix = numeric_df.corr()\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", square=True)\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-06-01T12:48:38.058449700Z"
    }
   },
   "id": "4198104465d53ee8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Checking which variables are most correlated with Loan_approval (only linear) - this is the Loan_Approval row in the heatmap\n",
    "numeric_df = train.select_dtypes(include='number')\n",
    "correlations = numeric_df.corr()['Loan_Approval'].sort_values(ascending=False)\n",
    "print(correlations)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-06-01T12:48:38.060444500Z"
    }
   },
   "id": "dd0e681b69d85555"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Key Insights from the heat map**\n",
    "C and E are highly correlated (r = 0.87), indicating redundancy.\n",
    "Neither has a strong linear correlation with the target, but E shows a slightly stronger signal (r = 0.125). Even though E has more missing values (as we will see later on), it’s still more useful, so we’ll keep E and drop C. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "18833f3e4451f9f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We checked for overlap between Employment_Type and Job_Type using a cross-tabulation heatmap.\n",
    "The results show no strong or consistent relationship between the two — most employment types are associated with a wide variety of job types.\n",
    "Therefore, we concluded that the two features capture distinct information and chose to keep both."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dbba21d75f846898"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "crosstab = pd.crosstab(train['Employment_Type'], train['Job_Type'], normalize='index')\n",
    "plt.figure(figsize=(14, 7))\n",
    "sns.heatmap(crosstab, cmap=\"Blues\", annot=True, fmt=\".2f\")\n",
    "plt.title(\"Employment Type vs Job Type\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-06-01T12:48:38.064433800Z"
    }
   },
   "id": "2c2ad593ba0cad71"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Changes based on the key insights:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "114b5f5a38536405"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# 1. Removing columns B and C\n",
    "train = train.drop(['B', 'C'], axis=1)\n",
    "test = test.drop(['B', 'C'], axis=1)\n",
    "n_columns -= 2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-01T12:49:48.391890700Z",
     "start_time": "2025-06-01T12:49:48.363781600Z"
    }
   },
   "id": "e60c0335a6987d0f"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# ------------ Grouping functions ------------\n",
    "\n",
    "# Grouping Preferred_Communication_Method\n",
    "def group_pref_com(pref):\n",
    "    if pref == \"Mail\" or pref == \"mail\":\n",
    "        return \"Mail\"\n",
    "    if pref == \"Email\" or pref == \"email\":\n",
    "        return \"EMail\"\n",
    "    if pref == \"Phone\" or pref == \"phone\" or pref == \"phone call\":\n",
    "        return \"Phone\"\n",
    "    else:\n",
    "        return \"Other\"\n",
    "\n",
    "# Grouping Country_of_Residence\n",
    "def group_country(country):\n",
    "    if country == \"United-States\":\n",
    "        return \"USA\"\n",
    "    elif country == \"Mexico\":\n",
    "        return \"Mexico\"\n",
    "    else:\n",
    "        return \"Other\"\n",
    "    \n",
    "# Grouping Education_Level\n",
    "def group_education(edu):\n",
    "    pre_school = ['Preschool']\n",
    "    school = ['1st-4th', '5th-6th', '9th', '10th', '11th', '12th', '7th-8th']\n",
    "    post_hs = ['HS-grad', 'Some-college']\n",
    "    assoc = ['Assoc-voc', 'Assoc-acdm']\n",
    "    higher = ['Masters', 'Bachelors']\n",
    "    postgraduate = ['Doctorate', 'Prof-school']\n",
    "    \n",
    "    if edu in pre_school:\n",
    "        return 'pre_school'\n",
    "    elif edu in school:\n",
    "        return 'school'\n",
    "    elif edu in post_hs:\n",
    "        return 'post_hs'\n",
    "    elif edu in assoc:\n",
    "        return 'assoc'\n",
    "    elif edu in higher:\n",
    "        return 'higher'\n",
    "    elif edu in postgraduate:\n",
    "        return 'postgraduate'\n",
    "    else:\n",
    "        return 'Other'\n",
    "    \n",
    "# Grouping Household_role\n",
    "def group_household_role(role):\n",
    "    if role in ['Husband', 'Wife']:\n",
    "        return 'Spouse'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "# Grouping Employment_Type\n",
    "def group_employment(emp):\n",
    "    if emp == 'Private':\n",
    "        return 'Private'\n",
    "    elif emp in ['Self-emp-not-inc', 'Self-emp-inc', 'Local-gov', 'State-gov', 'Federal-gov']:\n",
    "        return 'Not-Private'\n",
    "    elif emp in ['Without-pay', 'Never-worked']:\n",
    "        return 'Non-working'\n",
    "    else:\n",
    "        return 'Other'\n",
    "    \n",
    "# Grouping Ethnicity:\n",
    "def group_ethnicity(eth):\n",
    "    if eth == 'White':\n",
    "        return 'White'\n",
    "    elif eth == 'Black':\n",
    "        return 'Black'\n",
    "    else:\n",
    "        return 'Other'\n",
    "    \n",
    "# 1. Mapping dictionary for Years_of_Education\n",
    "edu_to_years = {\n",
    "    'Preschool'    : 0,\n",
    "    '1st-4th'      : (1 + 2 + 3 + 4) / 4,\n",
    "    '5th-6th'      : 5.5,\n",
    "    '7th-8th'      : 7.5,\n",
    "    '9th'          : 9,\n",
    "    '10th'         : 10,\n",
    "    '11th'         : 11,\n",
    "    '12th'         : 12,\n",
    "    'HS-grad'      : 12,\n",
    "    'Some-college' : (12 + 13 + 14 + 15) / 4,\n",
    "    'Assoc-voc'    : 14,\n",
    "    'Assoc-acdm'   : 14,\n",
    "    'Bachelors'    : 16,\n",
    "    'Masters'      : 18,\n",
    "    'Prof-school'  : 19,\n",
    "    'Doctorate'    : 20\n",
    "}\n",
    "\n",
    "\n",
    "def group_edu_years(X):\n",
    "    #X is a 2D array: [Years_of_Education, Education_Level\n",
    "    \n",
    "    # Convert columns to panda series to be able to map\n",
    "    years_of_education = pd.Series(X[:, 0])\n",
    "    education_level = pd.Series(X[:, 1])\n",
    "    \n",
    "    # Map Education_Level → numeric years\n",
    "    mapped_years = education_level.map(edu_to_years)\n",
    "    \n",
    "    # Fill NaN in the original 'years' with the mapped values\n",
    "    filled = years_of_education.fillna(mapped_years)\n",
    "    \n",
    "    # Return as a single-column DataFrame\n",
    "    return filled.to_frame(name='Years_of_Education')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-01T12:50:38.929287900Z",
     "start_time": "2025-06-01T12:50:38.888453400Z"
    }
   },
   "id": "ea59906540420095"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "# ------------ function transformers ------------\n",
    "\n",
    "pref_com_group = FunctionTransformer(\n",
    "    func=lambda X: pd.Series(X.ravel()).map(group_pref_com).to_frame(),\n",
    "    validate=False\n",
    ")\n",
    "\n",
    "country_group = FunctionTransformer(\n",
    "    func=lambda X: pd.Series(X.ravel()).map(group_country).to_frame(),\n",
    "    validate=False\n",
    ")\n",
    "\n",
    "edu_level_group = FunctionTransformer(\n",
    "    func=lambda X: pd.Series(X.ravel()).map(group_education).to_frame(),\n",
    "    validate=False\n",
    ")\n",
    "\n",
    "house_hold_group = FunctionTransformer(\n",
    "    func=lambda X: pd.Series(X.ravel()).map(group_household_role).to_frame(),\n",
    "    validate=False\n",
    ")\n",
    "\n",
    "emp_type_group = FunctionTransformer(\n",
    "    func=lambda X: pd.Series(X.ravel()).map(group_employment).to_frame(),\n",
    "    validate=False\n",
    ")\n",
    "\n",
    "ethnicity_group = FunctionTransformer(\n",
    "    func=lambda X: pd.Series(X.ravel()).map(group_ethnicity).to_frame(),\n",
    "    validate=False\n",
    ")\n",
    "\n",
    "edu_years_group = FunctionTransformer(\n",
    "    func=group_edu_years,\n",
    "    validate=False\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-01T12:50:45.488628Z",
     "start_time": "2025-06-01T12:50:45.444567200Z"
    }
   },
   "id": "323cef953dcfe6f8"
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "# ------------ pipelines ------------\n",
    "\n",
    "knn_imputer = KNNImputer(n_neighbors=3, weights='uniform')\n",
    "\n",
    "# Preferred_Communication_Method pipeline\n",
    "pref_pipeline = Pipeline([\n",
    "    ('group', pref_com_group),  \n",
    "    ('impute', SimpleImputer(strategy='constant', fill_value='Missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Country_of_Residence pipeline\n",
    "country_pipeline = Pipeline([\n",
    "    ('group', country_group),\n",
    "    ('impute', SimpleImputer(strategy='most_frequent')),  # or 'constant' if you prefer\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Education_Level pipeline\n",
    "edu_pipeline = Pipeline([\n",
    "    ('group', edu_level_group),\n",
    "    ('impute', SimpleImputer(strategy='constant', fill_value='Missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Employment_Type pipeline\n",
    "employment_pipeline = Pipeline([\n",
    "    ('group', emp_type_group),\n",
    "    ('impute', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Ethnicity pipeline\n",
    "ethnicity_pipeline = Pipeline([\n",
    "    ('group', ethnicity_group),\n",
    "    ('impute', SimpleImputer(strategy='constant', fill_value='Missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Household pipeline\n",
    "Household_pipeline = Pipeline([\n",
    "    ('group', house_hold_group),\n",
    "    ('impute', SimpleImputer(strategy='constant', fill_value='Missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Years of education pipeline\n",
    "edu_years_pipeline = Pipeline([\n",
    "    ('fill_years', edu_years_group),\n",
    "    ('impute_years', SimpleImputer(strategy='mean'))\n",
    "])\n",
    "\n",
    "\n",
    "# columns without special adjustments:\n",
    "categorical_col = ['Gender', 'Marital_Status', 'Job_Type', 'Employment_Type']\n",
    "numerical_col = ['E']\n",
    "skw_numerical_col = ['Age', 'Investment_Gain', 'Investment_Loss']\n",
    "\n",
    "# Pipeline for categorical features\n",
    "cat_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='Missing')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Pipeline for numerical features (mean imputation)\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean'))\n",
    "])\n",
    "\n",
    "# Pipeline for numerical features (median imputation) - for skewed columns\n",
    "skw_numerical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median'))\n",
    "])\n",
    "\n",
    "\n",
    "# Combine pipelines into a ColumnTransformer\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('pref',       pref_pipeline,         ['Preferred_Communication_Method']),\n",
    "    ('country',    country_pipeline,      ['Country_of_Residence']),\n",
    "    ('edu_level',  edu_pipeline,          ['Education_Level']),\n",
    "    ('edu_years',  edu_years_pipeline,    ['Years_of_Education', 'Education_Level']),\n",
    "    ('employment', employment_pipeline,    ['Employment_Type']),\n",
    "    ('ethnicity',  ethnicity_pipeline,    ['Ethnicity']),\n",
    "    ('household',  Household_pipeline,    ['Household_Role']),\n",
    "    ('cat_basic',  cat_pipeline,          ['Gender', 'Marital_Status']),                        # Other categorical columns\n",
    "    ('num_basic',  num_pipeline,          ['E']),                                               # Simple numeric column\n",
    "    ('skw_num',    skw_numerical_pipeline,['Age', 'Investment_Gain', 'Investment_Loss'])        # Skewed numeric columns\n",
    "])\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-02T05:47:52.141524500Z",
     "start_time": "2025-06-02T05:47:52.081384600Z"
    }
   },
   "id": "ba45e67358850d1a"
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total missing values after preprocessing: 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = train.drop(columns=['Loan_Approval'])\n",
    "X_transformed = preprocessor.fit_transform(X)\n",
    "\n",
    "\n",
    "# 3. Check for missing values\n",
    "print(\"Total missing values after preprocessing:\", np.isnan(X_transformed).sum())\n",
    "\n",
    "# Optional: break if something is wrong\n",
    "assert not np.isnan(X_transformed).any(), \"❌ Still missing values after preprocessing!\"\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-01T13:32:36.426867700Z",
     "start_time": "2025-06-01T13:32:36.346442600Z"
    }
   },
   "id": "a56ba939da7e6672"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "#Doing a log transformation while still distinguishing losses vs. gains.\n",
    "train['Net_Investment'] = train['Investment_Gain'] - train['Investment_Loss']\n",
    "test['Net_Investment'] = test['Investment_Gain'] - test['Investment_Loss']\n",
    "\n",
    "train['Log_Net_Investment'] = np.sign(train['Net_Investment']) * np.log2(abs(train['Net_Investment']) + 1)\n",
    "test['Log_Net_Investment'] = np.sign(test['Net_Investment']) * np.log2(abs(test['Net_Investment']) + 1)\n",
    "\n",
    "\n",
    "# Side-by-side boxplots \n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "sns.boxplot(x=train['Net_Investment'], ax=axes[0])\n",
    "axes[0].set_title('Original Net_Investment')\n",
    "axes[0].set_xlabel('Net_Investment')\n",
    "\n",
    "sns.boxplot(x=train['Log_Net_Investment'], ax=axes[1])\n",
    "axes[1].set_title('Log-Transformed Net_Investment')\n",
    "axes[1].set_xlabel('Log_Net_Investment')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Side-by-side histograms\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "sns.histplot(train['Net_Investment'].dropna(), kde=True, ax=axes[0])\n",
    "axes[0].set_title('Distribution of Net_Investment')\n",
    "axes[0].set_xlabel('Net_Investment')\n",
    "\n",
    "sns.histplot(train['Log_Net_Investment'].dropna(), kde=True, ax=axes[1])\n",
    "axes[1].set_title('Distribution of Log-Transformed Net_Investment')\n",
    "axes[1].set_xlabel('Log_Net_Investment')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-01T12:48:38.159956400Z",
     "start_time": "2025-06-01T12:48:38.095544100Z"
    }
   },
   "id": "572f1caebdf1dfa5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train['log_gain'] = np.log2(train['Investment_Gain'] + 1)\n",
    "train['log_loss'] = np.log2(train['Investment_Loss'] + 1)\n",
    "train['net_investment_log'] = train['log_gain'] - train['log_loss']\n",
    "\n",
    "plt.hist(train['net_investment_log'])\n",
    "plt.show()\n",
    "\n",
    "sns.histplot(train['net_investment_log'].dropna(), kde=True)\n",
    "plt.title(f\"Distribution of net_investment_log\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-06-01T12:48:38.098090800Z"
    }
   },
   "id": "bc94fa2c0e4a4e18"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We now have a few ways of dealing with the investments: \n",
    "1. keeping each one in the original way.\n",
    "2. keeping each one with a log transformation.\n",
    "3. Log on each one and then difference\n",
    "4. Difference and then log on each one.\n",
    "\n",
    "we will need to do some cross-validation and check which one gives the best results."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c4e7a8147e8d9782"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We applied a log transformation to Net_Investment to reduce skew and compress extreme outliers.\n",
    "The resulting distribution is more symmetric, so we will keep it.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dcc19d2573d9a90b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We applied a log transformation to column A to reduce skew and potentially improve its relationship with the target.\n",
    "However, the correlation with Loan_Approval remained low, so we won't use it."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dcaf1ff96686fb72"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pd.crosstab(train['Loan_Approval'], train['Gender'])\n",
    "pd.crosstab(train['Loan_Approval'], train['Ethnicity'], normalize='index')\n",
    "pd.crosstab(train['Loan_Approval'], train['Education_Level'], normalize='index')\n",
    "pd.crosstab(train['Loan_Approval_str'], train['Gender'], normalize='index')\n",
    "\n",
    "# Doesn't really help because of the unbalanced proportions. Add an explanation.\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-06-01T12:48:38.099523Z"
    }
   },
   "id": "ecc695970f3c9fe"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# AFTER CHANGING \n",
    "\n",
    "# Checking which variables are most correlated with Loan_approval (only linear):\n",
    "numeric_df = train.select_dtypes(include='number')\n",
    "correlations = numeric_df.corr()['Loan_Approval'].sort_values(ascending=False)\n",
    "print(correlations)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-06-01T12:48:38.101531600Z"
    }
   },
   "id": "1d1ac20a311400d4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Data Preprocessing\n",
    "_Handle missing values, encode categorical features, normalize, etc._\n",
    "\n",
    "**✏️ Answer in markdown:**\n",
    "- Did you normalize the data? Why?\n",
    "- How did you handle categorical variables?\n",
    "- Did you apply PCA or feature selection?\n",
    "- Did you drop or create new features?\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aa6ac0592772b405"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "######## TRY AGAIN AFTER ADDING AGE\n",
    "'''\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "\n",
    "df = train  \n",
    "\n",
    "# 1. Define features & split known vs missing\n",
    "features = ['Weekly_Work_Hours', 'Years_of_Education', 'Age']\n",
    "known   = df[df['Investment_Gain'].notnull() ].copy()\n",
    "missing = df[df['Investment_Gain'].isnull()].copy()\n",
    "\n",
    "X_known = known[features]\n",
    "y_known = known['Investment_Gain']\n",
    "\n",
    "# 2. Hold out 20% for validation\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    X_known, y_known,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 3. Train on the 80%\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_tr, y_tr)\n",
    "\n",
    "# 4. Predict on the hold‐out and evaluate\n",
    "y_pred = lr.predict(X_val)\n",
    "print(f\"R² on hold-out: {r2_score(y_val, y_pred):.3f}\")\n",
    "print(f\"MAE on hold-out: {mean_absolute_error(y_val, y_pred):.3f}\")\n",
    "\n",
    "# 5. (Optional) Retrain on ALL known data and impute missings\n",
    "lr_full = LinearRegression()\n",
    "lr_full.fit(X_known, y_known)\n",
    "\n",
    "X_missing = missing[features]\n",
    "df.loc[df['Investment_Gain'].isnull(), 'Investment_Gain'] = lr_full.predict(X_missing)\n",
    "'''\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-06-01T12:48:38.104579300Z"
    }
   },
   "id": "582f303fb88677b8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We tried to handle the missing values with linear regression, but R^2 was 0.02, so we just impute with the median."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1bb220032e4c5488"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The plot shows that column B is completely empty, so we should remove it. The rest of the columns have some missing values, but there doesn’t seem to be any clear pattern."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bfde1cd4ed56b68c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 1. Create a boolean mask for missingness - job type\n",
    "mask_Job_Type = train['Job_Type'].isna()            # True where missing\n",
    "train['MissingFlag_Job_Type'] = mask_Job_Type.map({True: 'Missing', False: 'Present'})\n",
    "\n",
    "# 2. Summary statistics for numeric vars\n",
    "numeric_cols = ['Log_Net_Investment']\n",
    "#numeric_cols = ['Age', 'Years_of_Education', 'Weekly_Work_Hours', 'Net_Investment', 'A', 'D', 'E']\n",
    "print(train.groupby('MissingFlag_Job_Type')[numeric_cols].describe().unstack(1))\n",
    "\n",
    "''' bring back after - just sick of the big graphs.\n",
    "# 4. Frequencies for other categorical vars\n",
    "cat_cols = ['Marital_Status', 'Household_Role', ]\n",
    "for col in cat_cols:\n",
    "    freq = pd.crosstab(train['MissingFlag_Job_Type'], train[col], normalize='index')\n",
    "    print(f'\\nRelative frequency of {col}:\\n', freq)\n",
    "    freq.plot.bar(stacked=True)\n",
    "    plt.title(f'{col} by missingness')\n",
    "    plt.ylabel('Proportion')\n",
    "    plt.show()\n",
    "\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-06-01T12:48:38.107918Z"
    }
   },
   "id": "90178773caacf0f3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 1. Create a boolean mask for missingness with Investment_Gain to see if we can delete it to use YJ\n",
    "mask_Investment_Gain = train['Investment_Gain'].isna()            # True where missing\n",
    "train['MissingFlag_Investment_Gain'] = mask_Investment_Gain.map({True: 'Missing', False: 'Present'})\n",
    "\n",
    "# 2. Summary statistics for numeric vars\n",
    "numeric_cols = ['E']\n",
    "#numeric_cols = ['Age', 'Years_of_Education', 'Weekly_Work_Hours', 'A', 'D', 'E']\n",
    "print(train.groupby('MissingFlag_Investment_Gain')[numeric_cols].describe().unstack(1))\n",
    "\n",
    "\n",
    "''' bring back after - just sick of the big graphs.\n",
    "\n",
    "# 4. Frequencies for other categorical vars\n",
    "cat_cols = ['Marital_Status', 'Household_Role', ]\n",
    "for col in cat_cols:\n",
    "    freq = pd.crosstab(train['MissingFlag_Investment_Gain'], train[col], normalize='index')\n",
    "    print(f'\\nRelative frequency of {col}:\\n', freq)\n",
    "    freq.plot.bar(stacked=True)\n",
    "    plt.title(f'{col} by missingness')\n",
    "    plt.ylabel('Proportion')\n",
    "    plt.show()\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-06-01T12:48:38.109077800Z"
    }
   },
   "id": "4ec05495035e859f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 1. Create a boolean mask for missingness with Ethnicity\n",
    "mask_Ethnicity = train['Ethnicity'].isna()            # True where missing\n",
    "train['MissingFlag_Ethnicity'] = mask_Ethnicity.map({True: 'Missing', False: 'Present'})\n",
    "\n",
    "# 2. Summary statistics for numeric vars\n",
    "numeric_cols = ['D']\n",
    "#numeric_cols = ['Age', 'Years_of_Education', 'Weekly_Work_Hours', 'A', 'D', 'E']\n",
    "print(train.groupby('MissingFlag_Ethnicity')[numeric_cols].describe().unstack(1))\n",
    "\n",
    "'''\n",
    "#bring back after - just sick of the big graphs.\n",
    "\n",
    "# 4. Frequencies for other categorical vars\n",
    "cat_cols = ['Marital_Status', 'Household_Role', 'Employment_Type', 'Job_Type', 'Gender', 'Country_of_Residence', 'Preferred_Communication_Method']\n",
    "for col in cat_cols:\n",
    "    freq = pd.crosstab(train['MissingFlag_Ethnicity'], train[col], normalize='index')\n",
    "    print(f'\\nRelative frequency of {col}:\\n', freq)\n",
    "    freq.plot.bar(stacked=True)\n",
    "    plt.title(f'{col} by missingness')\n",
    "    plt.ylabel('Proportion')\n",
    "    plt.show()\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-06-01T12:48:38.111104800Z"
    }
   },
   "id": "8bffe75d52c78054"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 1. Create a boolean mask for missingness with Ethnicity\n",
    "mask_Ethnicity = train['Preferred_Communication_Method'].isna()            # True where missing\n",
    "train['MissingFlag_Ethnicity'] = mask_Ethnicity.map({True: 'Missing', False: 'Present'})\n",
    "\n",
    "# 2. Summary statistics for numeric vars\n",
    "numeric_cols = ['Weekly_Work_Hours']\n",
    "#numeric_cols = ['Age', 'Years_of_Education', 'Weekly_Work_Hours', 'A', 'D', 'E']\n",
    "print(train.groupby('MissingFlag_Ethnicity')[numeric_cols].describe().unstack(1))\n",
    "\n",
    "'''\n",
    "#bring back after - just sick of the big graphs.\n",
    "\n",
    "# 4. Frequencies for other categorical vars\n",
    "cat_cols = ['Marital_Status', 'Household_Role', 'Employment_Type', 'Job_Type', 'Gender', 'Country_of_Residence', 'Preferred_Communication_Method']\n",
    "for col in cat_cols:\n",
    "    freq = pd.crosstab(train['MissingFlag_Ethnicity'], train[col], normalize='index')\n",
    "    print(f'\\nRelative frequency of {col}:\\n', freq)\n",
    "    freq.plot.bar(stacked=True)\n",
    "    plt.title(f'{col} by missingness')\n",
    "    plt.ylabel('Proportion')\n",
    "    plt.show()\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-06-01T12:48:38.112109400Z"
    }
   },
   "id": "ebcdeacd8c6f8288"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 1. Create a boolean mask for Job tyoe with Ethnicity\n",
    "mask_Ethnicity = train['Job_Type'].isna()            # True where missing\n",
    "train['MissingFlag_Ethnicity'] = mask_Ethnicity.map({True: 'Missing', False: 'Present'})\n",
    "\n",
    "# 2. Summary statistics for numeric vars\n",
    "numeric_cols = ['Weekly_Work_Hours']\n",
    "#numeric_cols = ['Age', 'Years_of_Education', 'Weekly_Work_Hours', 'A', 'D', 'E']\n",
    "print(train.groupby('MissingFlag_Ethnicity')[numeric_cols].describe().unstack(1))\n",
    "\n",
    "'''\n",
    "#bring back after - just sick of the big graphs.\n",
    "\n",
    "# 4. Frequencies for other categorical vars\n",
    "cat_cols = ['Marital_Status', 'Household_Role', 'Employment_Type', 'Job_Type', 'Gender', 'Country_of_Residence', 'Preferred_Communication_Method']\n",
    "for col in cat_cols:\n",
    "    freq = pd.crosstab(train['MissingFlag_Ethnicity'], train[col], normalize='index')\n",
    "    print(f'\\nRelative frequency of {col}:\\n', freq)\n",
    "    freq.plot.bar(stacked=True)\n",
    "    plt.title(f'{col} by missingness')\n",
    "    plt.ylabel('Proportion')\n",
    "    plt.show()\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-06-01T12:48:38.114495100Z"
    }
   },
   "id": "dc682854a7545428"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 1. Create a boolean mask for missingness with Ethnicity\n",
    "mask_Ethnicity = train['Employment_Type'].isna()            # True where missing\n",
    "train['MissingFlag_Ethnicity'] = mask_Ethnicity.map({True: 'Missing', False: 'Present'})\n",
    "\n",
    "# 2. Summary statistics for numeric vars\n",
    "numeric_cols = ['Weekly_Work_Hours']\n",
    "#numeric_cols = ['Age', 'Years_of_Education', 'Weekly_Work_Hours', 'A', 'D', 'E']\n",
    "print(train.groupby('MissingFlag_Ethnicity')[numeric_cols].describe().unstack(1))\n",
    "\n",
    "'''\n",
    "#bring back after - just sick of the big graphs.\n",
    "\n",
    "# 4. Frequencies for other categorical vars\n",
    "cat_cols = ['Marital_Status', 'Household_Role', 'Employment_Type', 'Job_Type', 'Gender', 'Country_of_Residence', 'Preferred_Communication_Method']\n",
    "for col in cat_cols:\n",
    "    freq = pd.crosstab(train['MissingFlag_Ethnicity'], train[col], normalize='index')\n",
    "    print(f'\\nRelative frequency of {col}:\\n', freq)\n",
    "    freq.plot.bar(stacked=True)\n",
    "    plt.title(f'{col} by missingness')\n",
    "    plt.ylabel('Proportion')\n",
    "    plt.show()\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-06-01T12:48:38.115570900Z"
    }
   },
   "id": "2f9d561e22c26d01"
  },
  {
   "cell_type": "markdown",
   "source": [
    "i see that the missing and present have a really different distribution. so i used knn"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "904a6fcd72dab2c4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We flagged missing Investment_Gain and saw identical “Missing” vs. “Present” distributions, so it’s MCAR. We will drop those rows to be able to apply Yeo–Johnson to the rest.- DONT THINK SO"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "edac95e97289ad40"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "\n",
    "a = train.loc[train['MissingFlag_Job_Type']=='Missing', 'Weekly_Work_Hours'].dropna()\n",
    "b = train.loc[train['MissingFlag_Job_Type']=='Present','Weekly_Work_Hours'].dropna()\n",
    "t_stat, p_val = ttest_ind(a, b, equal_var=False)\n",
    "print(f'p-value = {p_val}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-06-01T12:48:38.118046700Z"
    }
   },
   "id": "d43b1ed5929daff4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#notes:\n",
    "C is handeled with mean bacause its distributed normally."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "79660c63ed10f013"
  },
  {
   "cell_type": "markdown",
   "source": [
    "for values with more than 5 percent of missing data we checked if the data is missed randomly or not.\n",
    "we can see with the t-test that the missing job_type are not random. so we will use a more sofisticated way to deal with the missing values"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f136a7aed9a9fb2c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Training – Basic Models\n",
    "_Start simple. Train Logistic Regression or KNN._"
   ],
   "id": "85618933fc93ce16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-06-01T12:48:38.120068300Z"
    }
   },
   "id": "1b9e4e1a7e53ba54"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Training – Advanced Models\n",
    "Choose 3 models from:\n",
    "- Decision Tree\n",
    "- Random Forest or AdaBoost\n",
    "- Support Vector Machine\n",
    "- Multi-Layer Perceptron (Neural Net)\n",
    "\n",
    "_Include basic hyperparameter tuning or explain defaults._"
   ],
   "id": "ecc8b1eefecc948c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation\n",
    "- Build confusion matrix for at least one model\n",
    "- Generate ROC curves (using K-Fold CV)\n",
    "- Report AUC\n",
    "\n",
    "**✏️ Answer in markdown:**\n",
    "- Does any model overfit?\n",
    "- Which model generalizes best?\n",
    "- Final choice justification"
   ],
   "id": "65b470cc0d11e7b7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Final Prediction on Test Set\n",
    "_Predict probabilities (not binary labels!) and create submission file._"
   ],
   "id": "1f21f590332511f3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-06-01T12:48:38.121072800Z"
    }
   },
   "outputs": [],
   "source": [
    "# Example:\n",
    "# final_model.predict_proba(test)[:, 1]  # ← probability of 'approved'\n",
    "# submission = pd.DataFrame({\n",
    "#     'customer_id': test['customer_id'],\n",
    "#     'loan_approval': predictions\n",
    "# })\n",
    "# submission.to_csv('Submission_group_32.csv', index=False)"
   ],
   "id": "625f5bbc3680f72a"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Appendix / Extra Notes\n",
    "_Use this area for extra analysis, failed attempts, or ideas you explored._"
   ],
   "id": "ae46b3996890ae43"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
